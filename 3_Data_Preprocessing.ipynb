{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Observations\n",
    "The simplest way to deal with missing values, if your data can afford it, is to drop the whole observation when any missing value is found. You have to be careful not to drop too many: depending on the distribution of the missing values across your dataset it might severely affect the usability of your dataset. If, after dropping the rows, I end up with a very small dataset, or find that the reduction in data size is more than 50%, I start checking my data to see what features have the most holes in them and perhaps exclude those altogether; if a feature has most of its values missing (unless a missing value bears a meaning), from a modeling point of view, it is fairly useless.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"comm\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"join_df.csv\",inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Zip: integer (nullable = true)\n",
      " |-- ComplaintID: integer (nullable = true)\n",
      " |-- ProblemID: integer (nullable = true)\n",
      " |-- SpaceType: string (nullable = true)\n",
      " |-- TypeID: integer (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- MajorCategoryID: integer (nullable = true)\n",
      " |-- MajorCategory: string (nullable = true)\n",
      " |-- MinorCategoryID: integer (nullable = true)\n",
      " |-- MinorCategory: string (nullable = true)\n",
      " |-- CodeID: integer (nullable = true)\n",
      " |-- Code: string (nullable = true)\n",
      " |-- StatusDate: string (nullable = true)\n",
      " |-- ReceivedDate: string (nullable = true)\n",
      " |-- closeTime: double (nullable = true)\n",
      " |-- All Households: double (nullable = true)\n",
      " |-- Families: double (nullable = true)\n",
      " |-- Families with Children: double (nullable = true)\n",
      " |-- Families without Children: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|  Zip|ComplaintID|ProblemID|       SpaceType|TypeID|     Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|                Code|StatusDate|ReceivedDate|closeTime|All Households|Families|Families with Children|Families without Children|\n",
      "+-----+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|11432|    2397487|  3768602|ENTIRE APARTMENT|     2|HAZARDOUS|             13|     NONCONST|            106|       VERMIN|   886|             ROACHES|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432|    2397487|  3768603|           OTHER|     2|HAZARDOUS|             13|     NONCONST|            106|       VERMIN|   884|                MICE|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432|    2397487|  3768604| ENTIRE BUILDING|     2|HAZARDOUS|             28|PAINT/PLASTER|            198|         WALL|  1400|PAINT DIRTY AND U...|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "+-----+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To find the number of missing observations per row, we can use the following snippet: \n",
    "df_miss.rdd.map( lambda row: (row['id'], sum([c == None for c in row])) ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2397487, 0), (2397487, 0), (2397487, 0), (2397487, 0), (2397487, 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.map(lambda row: (row['ComplaintID'], sum([c==None for c in row]))).take(5) # .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3793"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sql, make dataframe accessible\n",
    "df.createOrReplaceTempView('zip')\n",
    "\n",
    "# this selects the 'Zip' column from the dataframe and saves it to the 'sqlDF' variable\n",
    "zipc = spark.sql(\"SELECT * FROM zip\") #.show(1)\n",
    "\n",
    "# .select() goes into the 'address' column (the only column in this reduced dataframe) and then selects the city from each row of this column\n",
    "zipcode = zipc.select('zip.Zip') #.show()\n",
    "\n",
    "# now that we have the cities isolated, we can return only the rows of the dataframe where the city is 'Houston'\n",
    "#houston = spark.sql(\"SELECT * FROM addr where address.city like '%Houston%'\") #.show()\n",
    "\n",
    "# also we need to use <<== 'Houston'>> instead of <<like '%[]%'>> to account for \"South Houston\"\n",
    "bedstuy = spark.sql(\"SELECT * FROM zip where Zip == '11216'\") #.show()\n",
    "#houston.show()\n",
    "bedstuy.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|  Zip|ComplaintID|ProblemID|       SpaceType|TypeID|     Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|                Code|StatusDate|ReceivedDate|closeTime|All Households|Families|Families with Children|Families without Children|\n",
      "+-----+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|11432|    2397487|  3768602|ENTIRE APARTMENT|     2|HAZARDOUS|             13|     NONCONST|            106|       VERMIN|   886|             ROACHES|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432|    2397487|  3768603|           OTHER|     2|HAZARDOUS|             13|     NONCONST|            106|       VERMIN|   884|                MICE|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432|    2397487|  3768604| ENTIRE BUILDING|     2|HAZARDOUS|             28|PAINT/PLASTER|            198|         WALL|  1400|PAINT DIRTY AND U...|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432|    2397487|  3768605|           OTHER|     1|EMERGENCY|             13|     NONCONST|            101|      RUBBISH|  1309|               OTHER|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432|    2397487|  3768606|        BATHROOM|     1|EMERGENCY|              9|     PLUMBING|             68| WATER SUPPLY|  1282|               OTHER|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432|    2397487|  3768607|ENTIRE APARTMENT|     2|HAZARDOUS|             28|PAINT/PLASTER|            198|         WALL|  1366|          LARGE HOLE|09/12/2007|  06/20/2004|   1179.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "+-----+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.ComplaintID == '2397487')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's now check what percentage of missing observations are there in each column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-----------------+-----------------+--------------+------------+-----------------------+---------------------+-----------------------+---------------------+--------------+------------+--------------------+--------------------+--------------------+----------------------+----------------+------------------------------+---------------------------------+\n",
      "|Zip_missing|ComplaintID_missing|ProblemID_missing|SpaceType_missing|TypeID_missing|Type_missing|MajorCategoryID_missing|MajorCategory_missing|MinorCategoryID_missing|MinorCategory_missing|CodeID_missing|Code_missing|  StatusDate_missing|ReceivedDate_missing|   closeTime_missing|All Households_missing|Families_missing|Families with Children_missing|Families without Children_missing|\n",
      "+-----------+-------------------+-----------------+-----------------+--------------+------------+-----------------------+---------------------+-----------------------+---------------------+--------------+------------+--------------------+--------------------+--------------------+----------------------+----------------+------------------------------+---------------------------------+\n",
      "|        0.0|                0.0|              0.0|              0.0|           0.0|         0.0|                    0.0|                  0.0|                    0.0|                  0.0|           0.0|         0.0|9.073487993960239E-5|                 0.0|9.073487993960239E-5|                   0.0|             0.0|          7.440260155048284E-4|                              0.0|\n",
      "+-----------+-------------------+-----------------+-----------------+--------------+------------+-----------------------+---------------------+-----------------------+---------------------+--------------+------------+--------------------+--------------------+--------------------+----------------------+----------------+------------------------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "\n",
    "\n",
    "df.agg(*[ (1 - (fn.count(c) / fn.count('*'))).alias(c + '_missing') for c in df.columns ]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The * argument to the .count(...) method (in place of a column name) instructs the method to count all rows. On the other hand, the * preceding the list declaration instructs the .agg(...) method to treat the list as a set of separate parameters passed to the function.\n",
    "\n",
    "### Only a few columns have missing data:\n",
    "StatusDate_missing <br>\n",
    "closeTime_missing <br>\n",
    "Families with Children_missing\n",
    "\n",
    "### Idea #1\n",
    "What this tells us is that we have some income data missing regarding Families with Children. <br>\n",
    "One solution could be taking the average of the \"All Households\", \"Families\", and \"Families without Children\" columns \n",
    "\n",
    "\n",
    "### Idea #2 \n",
    "We also have some StatusDate information missing which is resulting in some closeTime data being missing (since closeTime is based on StatusDate. An empty cell for StatusDate means that the the case has yet to be closed. <br>\n",
    "One solution to this could be replacing all missing values in StatusDate with the current date and then repeating that function everytime the dataframe is read in so that a complaint that is really open isn't saved with whatever the last date where the dataset was read in. \n",
    "\n",
    "... \n",
    "\n",
    "However, since the percentages ae so small, it might be best to just drop these specific rows from the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Zip</th>\n",
       "      <th>ComplaintID</th>\n",
       "      <th>ProblemID</th>\n",
       "      <th>SpaceType</th>\n",
       "      <th>TypeID</th>\n",
       "      <th>Type</th>\n",
       "      <th>MajorCategoryID</th>\n",
       "      <th>MajorCategory</th>\n",
       "      <th>MinorCategoryID</th>\n",
       "      <th>MinorCategory</th>\n",
       "      <th>CodeID</th>\n",
       "      <th>Code</th>\n",
       "      <th>StatusDate</th>\n",
       "      <th>ReceivedDate</th>\n",
       "      <th>closeTime</th>\n",
       "      <th>All Households</th>\n",
       "      <th>Families</th>\n",
       "      <th>Families with Children</th>\n",
       "      <th>Families without Children</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11432</td>\n",
       "      <td>2397487</td>\n",
       "      <td>3768602</td>\n",
       "      <td>ENTIRE APARTMENT</td>\n",
       "      <td>2</td>\n",
       "      <td>HAZARDOUS</td>\n",
       "      <td>13</td>\n",
       "      <td>NONCONST</td>\n",
       "      <td>106</td>\n",
       "      <td>VERMIN</td>\n",
       "      <td>886</td>\n",
       "      <td>ROACHES</td>\n",
       "      <td>08/12/2004</td>\n",
       "      <td>06/20/2004</td>\n",
       "      <td>53.0</td>\n",
       "      <td>62148.0</td>\n",
       "      <td>67198.0</td>\n",
       "      <td>55290.0</td>\n",
       "      <td>79468.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11432</td>\n",
       "      <td>2397487</td>\n",
       "      <td>3768603</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>2</td>\n",
       "      <td>HAZARDOUS</td>\n",
       "      <td>13</td>\n",
       "      <td>NONCONST</td>\n",
       "      <td>106</td>\n",
       "      <td>VERMIN</td>\n",
       "      <td>884</td>\n",
       "      <td>MICE</td>\n",
       "      <td>08/12/2004</td>\n",
       "      <td>06/20/2004</td>\n",
       "      <td>53.0</td>\n",
       "      <td>62148.0</td>\n",
       "      <td>67198.0</td>\n",
       "      <td>55290.0</td>\n",
       "      <td>79468.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11432</td>\n",
       "      <td>2397487</td>\n",
       "      <td>3768604</td>\n",
       "      <td>ENTIRE BUILDING</td>\n",
       "      <td>2</td>\n",
       "      <td>HAZARDOUS</td>\n",
       "      <td>28</td>\n",
       "      <td>PAINT/PLASTER</td>\n",
       "      <td>198</td>\n",
       "      <td>WALL</td>\n",
       "      <td>1400</td>\n",
       "      <td>PAINT DIRTY AND UNSANITARY</td>\n",
       "      <td>08/12/2004</td>\n",
       "      <td>06/20/2004</td>\n",
       "      <td>53.0</td>\n",
       "      <td>62148.0</td>\n",
       "      <td>67198.0</td>\n",
       "      <td>55290.0</td>\n",
       "      <td>79468.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11432</td>\n",
       "      <td>2397487</td>\n",
       "      <td>3768605</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>1</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>13</td>\n",
       "      <td>NONCONST</td>\n",
       "      <td>101</td>\n",
       "      <td>RUBBISH</td>\n",
       "      <td>1309</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>08/12/2004</td>\n",
       "      <td>06/20/2004</td>\n",
       "      <td>53.0</td>\n",
       "      <td>62148.0</td>\n",
       "      <td>67198.0</td>\n",
       "      <td>55290.0</td>\n",
       "      <td>79468.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11432</td>\n",
       "      <td>2397487</td>\n",
       "      <td>3768606</td>\n",
       "      <td>BATHROOM</td>\n",
       "      <td>1</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>9</td>\n",
       "      <td>PLUMBING</td>\n",
       "      <td>68</td>\n",
       "      <td>WATER SUPPLY</td>\n",
       "      <td>1282</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>08/12/2004</td>\n",
       "      <td>06/20/2004</td>\n",
       "      <td>53.0</td>\n",
       "      <td>62148.0</td>\n",
       "      <td>67198.0</td>\n",
       "      <td>55290.0</td>\n",
       "      <td>79468.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275523</th>\n",
       "      <td>10460</td>\n",
       "      <td>9076230</td>\n",
       "      <td>18593101</td>\n",
       "      <td>ENTRANCE/FOYER</td>\n",
       "      <td>1</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>56</td>\n",
       "      <td>DOOR/WINDOW</td>\n",
       "      <td>333</td>\n",
       "      <td>DOOR</td>\n",
       "      <td>2666</td>\n",
       "      <td>NEEDS KEY FOR EXIT</td>\n",
       "      <td>06/10/2018</td>\n",
       "      <td>05/31/2018</td>\n",
       "      <td>10.0</td>\n",
       "      <td>27545.0</td>\n",
       "      <td>33282.0</td>\n",
       "      <td>25665.0</td>\n",
       "      <td>44649.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275524</th>\n",
       "      <td>10460</td>\n",
       "      <td>9076230</td>\n",
       "      <td>18593102</td>\n",
       "      <td>ENTIRE APARTMENT</td>\n",
       "      <td>1</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>10</td>\n",
       "      <td>ELECTRIC</td>\n",
       "      <td>71</td>\n",
       "      <td>WIRING</td>\n",
       "      <td>2461</td>\n",
       "      <td>ILLEGAL</td>\n",
       "      <td>06/10/2018</td>\n",
       "      <td>05/31/2018</td>\n",
       "      <td>10.0</td>\n",
       "      <td>27545.0</td>\n",
       "      <td>33282.0</td>\n",
       "      <td>25665.0</td>\n",
       "      <td>44649.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275525</th>\n",
       "      <td>10460</td>\n",
       "      <td>9076230</td>\n",
       "      <td>18593103</td>\n",
       "      <td>KITCHEN</td>\n",
       "      <td>3</td>\n",
       "      <td>NON EMERGENCY</td>\n",
       "      <td>8</td>\n",
       "      <td>APPLIANCE</td>\n",
       "      <td>59</td>\n",
       "      <td>ELECTRIC/GAS RANGE</td>\n",
       "      <td>2447</td>\n",
       "      <td>PILOT LIGHT OUT</td>\n",
       "      <td>06/10/2018</td>\n",
       "      <td>05/31/2018</td>\n",
       "      <td>10.0</td>\n",
       "      <td>27545.0</td>\n",
       "      <td>33282.0</td>\n",
       "      <td>25665.0</td>\n",
       "      <td>44649.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275526</th>\n",
       "      <td>10460</td>\n",
       "      <td>9076230</td>\n",
       "      <td>18593104</td>\n",
       "      <td>ENTIRE APARTMENT</td>\n",
       "      <td>3</td>\n",
       "      <td>NON EMERGENCY</td>\n",
       "      <td>56</td>\n",
       "      <td>DOOR/WINDOW</td>\n",
       "      <td>337</td>\n",
       "      <td>WINDOW FRAME</td>\n",
       "      <td>2672</td>\n",
       "      <td>WINDOW STUCK CLOSED OR OPEN</td>\n",
       "      <td>06/10/2018</td>\n",
       "      <td>05/31/2018</td>\n",
       "      <td>10.0</td>\n",
       "      <td>27545.0</td>\n",
       "      <td>33282.0</td>\n",
       "      <td>25665.0</td>\n",
       "      <td>44649.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275527</th>\n",
       "      <td>10460</td>\n",
       "      <td>9076230</td>\n",
       "      <td>18593105</td>\n",
       "      <td>ENTIRE APARTMENT</td>\n",
       "      <td>3</td>\n",
       "      <td>NON EMERGENCY</td>\n",
       "      <td>28</td>\n",
       "      <td>PAINT/PLASTER</td>\n",
       "      <td>198</td>\n",
       "      <td>WALL</td>\n",
       "      <td>2529</td>\n",
       "      <td>CHIPPED/PEELING/FLAKING</td>\n",
       "      <td>06/10/2018</td>\n",
       "      <td>05/31/2018</td>\n",
       "      <td>10.0</td>\n",
       "      <td>27545.0</td>\n",
       "      <td>33282.0</td>\n",
       "      <td>25665.0</td>\n",
       "      <td>44649.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275528 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Zip  ComplaintID  ProblemID         SpaceType  TypeID  \\\n",
       "0       11432      2397487    3768602  ENTIRE APARTMENT       2   \n",
       "1       11432      2397487    3768603             OTHER       2   \n",
       "2       11432      2397487    3768604   ENTIRE BUILDING       2   \n",
       "3       11432      2397487    3768605             OTHER       1   \n",
       "4       11432      2397487    3768606          BATHROOM       1   \n",
       "...       ...          ...        ...               ...     ...   \n",
       "275523  10460      9076230   18593101    ENTRANCE/FOYER       1   \n",
       "275524  10460      9076230   18593102  ENTIRE APARTMENT       1   \n",
       "275525  10460      9076230   18593103           KITCHEN       3   \n",
       "275526  10460      9076230   18593104  ENTIRE APARTMENT       3   \n",
       "275527  10460      9076230   18593105  ENTIRE APARTMENT       3   \n",
       "\n",
       "                 Type  MajorCategoryID  MajorCategory  MinorCategoryID  \\\n",
       "0           HAZARDOUS               13       NONCONST              106   \n",
       "1           HAZARDOUS               13       NONCONST              106   \n",
       "2           HAZARDOUS               28  PAINT/PLASTER              198   \n",
       "3           EMERGENCY               13       NONCONST              101   \n",
       "4           EMERGENCY                9       PLUMBING               68   \n",
       "...               ...              ...            ...              ...   \n",
       "275523      EMERGENCY               56    DOOR/WINDOW              333   \n",
       "275524      EMERGENCY               10       ELECTRIC               71   \n",
       "275525  NON EMERGENCY                8      APPLIANCE               59   \n",
       "275526  NON EMERGENCY               56    DOOR/WINDOW              337   \n",
       "275527  NON EMERGENCY               28  PAINT/PLASTER              198   \n",
       "\n",
       "             MinorCategory  CodeID                         Code  StatusDate  \\\n",
       "0                   VERMIN     886                      ROACHES  08/12/2004   \n",
       "1                   VERMIN     884                         MICE  08/12/2004   \n",
       "2                     WALL    1400   PAINT DIRTY AND UNSANITARY  08/12/2004   \n",
       "3                  RUBBISH    1309                        OTHER  08/12/2004   \n",
       "4             WATER SUPPLY    1282                        OTHER  08/12/2004   \n",
       "...                    ...     ...                          ...         ...   \n",
       "275523                DOOR    2666           NEEDS KEY FOR EXIT  06/10/2018   \n",
       "275524              WIRING    2461                      ILLEGAL  06/10/2018   \n",
       "275525  ELECTRIC/GAS RANGE    2447              PILOT LIGHT OUT  06/10/2018   \n",
       "275526        WINDOW FRAME    2672  WINDOW STUCK CLOSED OR OPEN  06/10/2018   \n",
       "275527                WALL    2529      CHIPPED/PEELING/FLAKING  06/10/2018   \n",
       "\n",
       "       ReceivedDate  closeTime  All Households  Families  \\\n",
       "0        06/20/2004       53.0         62148.0   67198.0   \n",
       "1        06/20/2004       53.0         62148.0   67198.0   \n",
       "2        06/20/2004       53.0         62148.0   67198.0   \n",
       "3        06/20/2004       53.0         62148.0   67198.0   \n",
       "4        06/20/2004       53.0         62148.0   67198.0   \n",
       "...             ...        ...             ...       ...   \n",
       "275523   05/31/2018       10.0         27545.0   33282.0   \n",
       "275524   05/31/2018       10.0         27545.0   33282.0   \n",
       "275525   05/31/2018       10.0         27545.0   33282.0   \n",
       "275526   05/31/2018       10.0         27545.0   33282.0   \n",
       "275527   05/31/2018       10.0         27545.0   33282.0   \n",
       "\n",
       "        Families with Children  Families without Children  \n",
       "0                      55290.0                    79468.0  \n",
       "1                      55290.0                    79468.0  \n",
       "2                      55290.0                    79468.0  \n",
       "3                      55290.0                    79468.0  \n",
       "4                      55290.0                    79468.0  \n",
       "...                        ...                        ...  \n",
       "275523                 25665.0                    44649.0  \n",
       "275524                 25665.0                    44649.0  \n",
       "275525                 25665.0                    44649.0  \n",
       "275526                 25665.0                    44649.0  \n",
       "275527                 25665.0                    44649.0  \n",
       "\n",
       "[275528 rows x 19 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df_pd = df.toPandas()\n",
    "df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'04/16/2021'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# today's date\n",
    "\n",
    "import datetime as dt     \n",
    "\n",
    "today = dt.datetime.today().strftime(\"%m/%d/%Y\")\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021',\n",
       " '04/16/2021']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list comprehsion to return the complaints that have not been closed\n",
    "# the entry for StatusDate will not have a date inside it, it will be 'nan'\n",
    "# [x for x in join_df['StatusDate'] if type(x) != str]\n",
    "\n",
    "\n",
    "# then use str(x).replace(str(x),today) to add the \n",
    "[str(x).replace(str(x),today) for x in df_pd['StatusDate'] if type(x) != str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n",
      "04/16/2021\n"
     ]
    }
   ],
   "source": [
    "for row in df_pd['StatusDate']:\n",
    "    if type(row) != str:\n",
    "        #print(str(row).replace(str(row),today))\n",
    "        row = str(row).replace(str(row),today)\n",
    "        print(row)\n",
    "        df_pd.loc[row,'StatusDate'] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'08/12/2004'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd['StatusDate'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x for x in join_df['StatusDate'] if type(x) != str]\n",
    "\n",
    "for row in range(len(df_pd['StatusDate'])-1):\n",
    "    if type(df_pd['StatusDate'][row]) != str:\n",
    "        #df_pd['StatusDate'][row] = \\\n",
    "        df_pd.iloc[row,'StatusDate'] = \\\n",
    "        str(df_pd['StatusDate'][row]).\\\n",
    "        replace(str(df_pd['StatusDate'][row]),today)\n",
    "        \n",
    "        \n",
    "        #print(str(row).replace(str(row),today))\n",
    "        #join_df.iloc[row,'StatusDate'] = \\\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join_df.iloc[5:25,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in df_pd['StatusDate'] if type(x) != str]\n",
    "\n",
    "# nice, we did it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now export it \n",
    "df_pd.to_csv(\"join_df_Mod4_April_16.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now read it back in and move on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now bring it back thru pyspark\n",
    "df = spark.read.csv(\"join_df_Mod4_April_16.csv\",inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|    Zip|ComplaintID|ProblemID|       SpaceType|TypeID|     Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|                Code|StatusDate|ReceivedDate|closeTime|All Households|Families|Families with Children|Families without Children|\n",
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|11432.0|  2397487.0|3768602.0|ENTIRE APARTMENT|   2.0|HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 886.0|             ROACHES|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|3768603.0|           OTHER|   2.0|HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 884.0|                MICE|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|3768604.0| ENTIRE BUILDING|   2.0|HAZARDOUS|           28.0|PAINT/PLASTER|          198.0|         WALL|1400.0|PAINT DIRTY AND U...|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|3768605.0|           OTHER|   1.0|EMERGENCY|           13.0|     NONCONST|          101.0|      RUBBISH|1309.0|               OTHER|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|3768606.0|        BATHROOM|   1.0|EMERGENCY|            9.0|     PLUMBING|           68.0| WATER SUPPLY|1282.0|               OTHER|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Editing the time stamps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|    Zip|ComplaintID|ProblemID|       SpaceType|TypeID|     Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|                Code|StatusDate|ReceivedDate|closeTime|All Households|Families|Families with Children|Families without Children|\n",
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|11432.0|  2397487.0|3768602.0|ENTIRE APARTMENT|   2.0|HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 886.0|             ROACHES|08/12/2004|  06/20/2004|       53|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|3768603.0|           OTHER|   2.0|HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 884.0|                MICE|08/12/2004|  06/20/2004|       53|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|3768604.0| ENTIRE BUILDING|   2.0|HAZARDOUS|           28.0|PAINT/PLASTER|          198.0|         WALL|1400.0|PAINT DIRTY AND U...|08/12/2004|  06/20/2004|       53|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|3768605.0|           OTHER|   1.0|EMERGENCY|           13.0|     NONCONST|          101.0|      RUBBISH|1309.0|               OTHER|08/12/2004|  06/20/2004|       53|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|3768606.0|        BATHROOM|   1.0|EMERGENCY|            9.0|     PLUMBING|           68.0| WATER SUPPLY|1282.0|               OTHER|08/12/2004|  06/20/2004|       53|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "# preprocessing: convert dates to datetime format and calculate response variable - closing time\n",
    "timeDiff = (unix_timestamp('StatusDate', 'MM/dd/yyyy') - unix_timestamp('ReceivedDate', 'MM/dd/yyyy')) / 86400 #seconds per day\n",
    "timeDiff = timeDiff.cast(IntegerType())\n",
    "\n",
    "# add closing time (time for complaint to be resolved) to dataframe\n",
    "df = df.withColumn(\"closeTime\", timeDiff)\n",
    "df.show(5)\n",
    "#Reference: https://stackoverflow.com/questions/47701339/subtracting-two-date-columns-in-pyspark-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I also need to change the contents of the Zip column in 'df' to be integers again instead of floating point numbers\n",
    "# for some reason that didn't save from join_income_opendatanyc.ipynb\n",
    "\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-cast-column-type/\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-rename-dataframe-column/\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df = df.withColumn(\"Zip\", df.Zip.cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|  Zip|ComplaintID|ProblemID|       SpaceType|TypeID|     Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|                Code|StatusDate|ReceivedDate|closeTime|All Households|Families|Families with Children|Families without Children|\n",
      "+-----+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|11432|  2397487.0|3768602.0|ENTIRE APARTMENT|   2.0|HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 886.0|             ROACHES|08/12/2004|  06/20/2004|       53|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432|  2397487.0|3768603.0|           OTHER|   2.0|HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 884.0|                MICE|08/12/2004|  06/20/2004|       53|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432|  2397487.0|3768604.0| ENTIRE BUILDING|   2.0|HAZARDOUS|           28.0|PAINT/PLASTER|          198.0|         WALL|1400.0|PAINT DIRTY AND U...|08/12/2004|  06/20/2004|       53|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432|  2397487.0|3768605.0|           OTHER|   1.0|EMERGENCY|           13.0|     NONCONST|          101.0|      RUBBISH|1309.0|               OTHER|08/12/2004|  06/20/2004|       53|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432|  2397487.0|3768606.0|        BATHROOM|   1.0|EMERGENCY|            9.0|     PLUMBING|           68.0| WATER SUPPLY|1282.0|               OTHER|08/12/2004|  06/20/2004|       53|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "+-----+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+---------------------+-----------------------+---------------------+--------------------+--------------------+------------------+--------------------+--------------------+----------------------+--------------------+------------------------------+---------------------------------+\n",
      "|         Zip_missing| ComplaintID_missing|   ProblemID_missing|   SpaceType_missing|      TypeID_missing|        Type_missing|MajorCategoryID_missing|MajorCategory_missing|MinorCategoryID_missing|MinorCategory_missing|      CodeID_missing|        Code_missing|StatusDate_missing|ReceivedDate_missing|   closeTime_missing|All Households_missing|    Families_missing|Families with Children_missing|Families without Children_missing|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+---------------------+-----------------------+---------------------+--------------------+--------------------+------------------+--------------------+--------------------+----------------------+--------------------+------------------------------+---------------------------------+\n",
      "|3.629382025138738...|3.629382025138738...|3.629382025138738...|3.629382025138738...|3.629382025138738...|3.629382025138738...|   3.629382025138738...| 3.629382025138738...|   3.629382025138738...| 3.629382025138738...|3.629382025138738...|3.629382025138738...|               0.0|3.629382025138738...|3.629382025138738...|  3.629382025138738...|3.629382025138738...|          7.476526971752495E-4|             3.629382025138738...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+---------------------+-----------------------+---------------------+--------------------+--------------------+------------------+--------------------+--------------------+----------------------+--------------------+------------------------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# double check to make sure nothing is missing \n",
    "\n",
    "import pyspark.sql.functions as fn\n",
    "\n",
    "df.agg(*[ (1 - (fn.count(c) / fn.count('*'))).alias(c + '_missing') for c in df.columns ]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+----------------------+-------------------------+\n",
      "|                 Zip|         ComplaintID|           ProblemID|           SpaceType|              TypeID|                Type|     MajorCategoryID|       MajorCategory|     MinorCategoryID|       MinorCategory|              CodeID|                Code|StatusDate|        ReceivedDate|           closeTime|      All Households|            Families|Families with Children|Families without Children|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+----------------------+-------------------------+\n",
      "|3.629382025138738...|3.629382025138738...|3.629382025138738...|3.629382025138738...|3.629382025138738...|3.629382025138738...|3.629382025138738...|3.629382025138738...|3.629382025138738...|3.629382025138738...|3.629382025138738...|3.629382025138738...|       0.0|3.629382025138738...|3.629382025138738...|3.629382025138738...|3.629382025138738...|  7.476526971752495E-4|     3.629382025138738...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+----------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(*[ (1 - (fn.count(c) / fn.count('*'))).alias(c) for c in df.columns ]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# oh god what have i done\n",
    "\n",
    "can i just drop null and move on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+---------+---------+------+----+---------------+-------------+---------------+-------------+------+----+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "| Zip|ComplaintID|ProblemID|SpaceType|TypeID|Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|Code|StatusDate|ReceivedDate|closeTime|All Households|Families|Families with Children|Families without Children|\n",
      "+----+-----------+---------+---------+------+----+---------------+-------------+---------------+-------------+------+----+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|null|       null|     null|     null|  null|null|           null|         null|           null|         null|  null|null|04/16/2021|        null|     null|          null|    null|                  null|                     null|\n",
      "+----+-----------+---------+---------+------+----+---------------+-------------+---------------+-------------+------+----+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create view so we can query using SQL\n",
    "df.createOrReplaceTempView(\"df_view\")\n",
    "\n",
    "spark.sql(\"select * from df_view where MajorCategory is Null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-drop-rows-with-null-values/#:~:text=In%20order%20to%20remove%20Rows,NULL%20values%20to%20delete%20rows.\n",
    "\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+------+----+---------------+-------------+---------------+-------------+------+----+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|Zip|ComplaintID|ProblemID|SpaceType|TypeID|Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|Code|StatusDate|ReceivedDate|closeTime|All Households|Families|Families with Children|Families without Children|\n",
      "+---+-----------+---------+---------+------+----+---------------+-------------+---------------+-------------+------+----+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "+---+-----------+---------+---------+------+----+---------------+-------------+---------------+-------------+------+----+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create view so we can query using SQL\n",
    "df.createOrReplaceTempView(\"df_view\")\n",
    "\n",
    "spark.sql(\"select * from df_view where MajorCategory is Null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+------+----+---------------+-------------+---------------+-------------+------+----+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|Zip|ComplaintID|ProblemID|SpaceType|TypeID|Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|Code|StatusDate|ReceivedDate|closeTime|All Households|Families|Families with Children|Families without Children|\n",
      "+---+-----------+---------+---------+------+----+---------------+-------------+---------------+-------------+------+----+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|0.0|        0.0|      0.0|      0.0|   0.0| 0.0|            0.0|          0.0|            0.0|          0.0|   0.0| 0.0|       0.0|         0.0|      0.0|           0.0|     0.0|                   0.0|                      0.0|\n",
      "+---+-----------+---------+---------+------+----+---------------+-------------+---------------+-------------+------+----+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(*[ (1 - (fn.count(c) / fn.count('*'))).alias(c) for c in df.columns ]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YES! FINALLY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK NOW EXPORT AND MOVE ON \n",
    "df.toPandas().to_csv(\"join_df_Mod4_April_16_edit1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More exploratory data analysis (specifically for the numerical columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+----------------------+-------------------------+\n",
      "|summary|         closeTime|   All Households|         Families|Families with Children|Families without Children|\n",
      "+-------+------------------+-----------------+-----------------+----------------------+-------------------------+\n",
      "|  count|            275323|           275323|           275323|                275323|                   275323|\n",
      "|   mean|13.838909208456975|54599.59264936093|66023.01643160942|    62668.236536722325|        70307.02262070368|\n",
      "| stddev| 42.05641735917821| 24634.0091019443|39975.16494195123|     48919.94228814118|        33756.22433021968|\n",
      "|    min|            -168.0|          21447.0|          25429.0|               20114.0|                  29907.0|\n",
      "|    max|            3372.0|         250001.0|         250001.0|              250001.0|                 250001.0|\n",
      "+-------+------------------+-----------------+-----------------+----------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numerical = ['closeTime','All Households','Families','Families with Children','Families without Children']\n",
    "desc = df.describe(numerical)\n",
    "desc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "Ah. The Income columns do not contain numbers, they contain strings representing the values of income. We should change that so we can run some analysis on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations\n",
    "Calculating correlations in PySpark is very easy once your data is in a DataFrame form. The only difficulties are that the .corr(...) method supports the Pearson correlation coefficient at the moment, and it can only calculate pairwise correlations, such as the following: fraud_df.corr('balance', 'numTrans') \n",
    "\n",
    "In order to create a correlations matrix, you can use the following script: \n",
    "n_numerical = len(numerical)\n",
    "corr = []\n",
    "for i in range(0, n_numerical):\n",
    "    temp = [None] * i\n",
    "    for j in range(i, n_numerical):\n",
    "        temp.append(fraud_df.corr(numerical[i], numerical[j]))\n",
    "    corr.append(temp)\n",
    "\n",
    "*Note: click into this cell to see the formatting*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograms\n",
    "Histograms are by far the easiest way to visually gauge the distribution of your features. There are three ways you can generate histograms in PySpark (or a Jupyter notebook):\n",
    "1. Aggregate the data in workers and return an aggregated list of bins and counts in each bin of the histogram to the driver\n",
    "2. Return all the data points to the driver and allow the plotting libraries' methods to do the job for you\n",
    "3. Sample your data and then return them to the driver for plotting.\n",
    "\n",
    "hists = fraud_df.select('balance').rdd.flatMap(\n",
    "lambda row: row\n",
    ").histogram(20)\n",
    "\n",
    "\n",
    "In a similar manner, a histogram can be created with Bokeh:\n",
    "b_hist = chrt.Bar(\n",
    "Data,\n",
    "values='freq', label='bins',\n",
    "title='Histogram of \\'balance\\'')\n",
    "chrt.show(b_hist)\n",
    "\n",
    "Since Bokeh uses D3.js in the background, the resulting chart is interactive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hists = df.select('closeTime').rdd.flatMap(lambda row: row).histogram(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x = df['closeTime']\n",
    "# bins = np.arange(0, 100, 5.0)\n",
    "\n",
    "# plt.figure(figsize=(10,8))\n",
    "# # the histogram of the data\n",
    "# plt.hist(x, bins, alpha=0.8, histtype='bar', color='gold',ec='black',weights=np.zeros_like(x) + 100. / x.size)\n",
    "\n",
    "# plt.xlabel('closeTime')\n",
    "# plt.ylabel('percentage')\n",
    "# plt.xticks(bins)\n",
    "# plt.show()\n",
    "\n",
    "# fig.savefig('closeTime'+\".pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "well that didn't go as planned..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now going through data_preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"data preprocessing\") \\\n",
    "    .config(\"spark.executor.memory\", '8g') \\\n",
    "    .config('spark.executor.cores', '4') \\\n",
    "    .config('spark.cores.max', '4') \\\n",
    "    .config(\"spark.driver.memory\",'8g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.host', 'udc-aw29-24a'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1618569865754'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.name', 'comm'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.port', '38694')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"join_df_Mod4_April_16_edit1.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE:\n",
    "You can read this data into a Spark Dataframe by issuing: <br>\n",
    "fraud = spark.read.csv(data_file, header='true', inferSchema='true')\n",
    "\n",
    "You can also read it in as an RDD and convert to Spark Dataframe using textFile()\n",
    "\n",
    "The first approach is recommended here since it's easier, but I walk you through the second approach as it illustrates more functionality. <br>\n",
    "There may be times when it makes sense to work with the RDD first and then convert to Dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data as RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "df = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the data type...it's pyspark.rdd.RDD, and we know that because we called textFile() to read the csv in as a RDD\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zip,ComplaintID,ProblemID,SpaceType,TypeID,Type,MajorCategoryID,MajorCategory,MinorCategoryID,MinorCategory,CodeID,Code,StatusDate,ReceivedDate,closeTime,All Households,Families,Families with Children,Families without Children',\n",
       " '11432.0,2397487.0,3768602.0,ENTIRE APARTMENT,2.0,HAZARDOUS,13.0,NONCONST,106.0,VERMIN,886.0,ROACHES,08/12/2004,06/20/2004,53.0,62148.0,67198.0,55290.0,79468.0',\n",
       " '11432.0,2397487.0,3768603.0,OTHER,2.0,HAZARDOUS,13.0,NONCONST,106.0,VERMIN,884.0,MICE,08/12/2004,06/20/2004,53.0,62148.0,67198.0,55290.0,79468.0',\n",
       " '11432.0,2397487.0,3768604.0,ENTIRE BUILDING,2.0,HAZARDOUS,28.0,PAINT/PLASTER,198.0,WALL,1400.0,PAINT DIRTY AND UNSANITARY,08/12/2004,06/20/2004,53.0,62148.0,67198.0,55290.0,79468.0',\n",
       " '11432.0,2397487.0,3768605.0,OTHER,1.0,EMERGENCY,13.0,NONCONST,101.0,RUBBISH,1309.0,OTHER,08/12/2004,06/20/2004,53.0,62148.0,67198.0,55290.0,79468.0']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the first 5 records\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the first record, which contains the header\n",
    "header = df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zip,ComplaintID,ProblemID,SpaceType,TypeID,Type,MajorCategoryID,MajorCategory,MinorCategoryID,MinorCategory,CodeID,Code,StatusDate,ReceivedDate,closeTime,All Households,Families,Families with Children,Families without Children'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275324"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a record count\n",
    "n_rec = df.count()\n",
    "n_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the comma-separated data in an RDD, skipping the header row. \n",
    "df = df.filter(lambda row: row != header).map(lambda row: [elem for elem in row.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['11432.0',\n",
       "  '2397487.0',\n",
       "  '3768602.0',\n",
       "  'ENTIRE APARTMENT',\n",
       "  '2.0',\n",
       "  'HAZARDOUS',\n",
       "  '13.0',\n",
       "  'NONCONST',\n",
       "  '106.0',\n",
       "  'VERMIN',\n",
       "  '886.0',\n",
       "  'ROACHES',\n",
       "  '08/12/2004',\n",
       "  '06/20/2004',\n",
       "  '53.0',\n",
       "  '62148.0',\n",
       "  '67198.0',\n",
       "  '55290.0',\n",
       "  '79468.0'],\n",
       " ['11432.0',\n",
       "  '2397487.0',\n",
       "  '3768603.0',\n",
       "  'OTHER',\n",
       "  '2.0',\n",
       "  'HAZARDOUS',\n",
       "  '13.0',\n",
       "  'NONCONST',\n",
       "  '106.0',\n",
       "  'VERMIN',\n",
       "  '884.0',\n",
       "  'MICE',\n",
       "  '08/12/2004',\n",
       "  '06/20/2004',\n",
       "  '53.0',\n",
       "  '62148.0',\n",
       "  '67198.0',\n",
       "  '55290.0',\n",
       "  '79468.0']]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDDs don't make sense here...I'm just going to import as DataFrame and move on with my life "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"join_df_Mod4_April_16_edit1.csv\",inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Zip: double (nullable = true)\n",
      " |-- ComplaintID: double (nullable = true)\n",
      " |-- ProblemID: double (nullable = true)\n",
      " |-- SpaceType: string (nullable = true)\n",
      " |-- TypeID: double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- MajorCategoryID: double (nullable = true)\n",
      " |-- MajorCategory: string (nullable = true)\n",
      " |-- MinorCategoryID: double (nullable = true)\n",
      " |-- MinorCategory: string (nullable = true)\n",
      " |-- CodeID: double (nullable = true)\n",
      " |-- Code: string (nullable = true)\n",
      " |-- StatusDate: string (nullable = true)\n",
      " |-- ReceivedDate: string (nullable = true)\n",
      " |-- closeTime: double (nullable = true)\n",
      " |-- All Households: double (nullable = true)\n",
      " |-- Families: double (nullable = true)\n",
      " |-- Families with Children: double (nullable = true)\n",
      " |-- Families without Children: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|    Zip|ComplaintID|ProblemID|       SpaceType|TypeID|     Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|                Code|StatusDate|ReceivedDate|closeTime|All Households|Families|Families with Children|Families without Children|\n",
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|11432.0|  2397487.0|3768602.0|ENTIRE APARTMENT|   2.0|HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 886.0|             ROACHES|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|3768603.0|           OTHER|   2.0|HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 884.0|                MICE|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|3768604.0| ENTIRE BUILDING|   2.0|HAZARDOUS|           28.0|PAINT/PLASTER|          198.0|         WALL|1400.0|PAINT DIRTY AND U...|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|3768605.0|           OTHER|   1.0|EMERGENCY|           13.0|     NONCONST|          101.0|      RUBBISH|1309.0|               OTHER|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|3768606.0|        BATHROOM|   1.0|EMERGENCY|            9.0|     PLUMBING|           68.0| WATER SUPPLY|1282.0|               OTHER|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create view so we can query using SQL\n",
    "df.createOrReplaceTempView(\"df_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  275323|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of using SQL to count records\n",
    "spark.sql(\"select count(*) from df_view\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+----------------+------+-------------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|    Zip|ComplaintID|  ProblemID|       SpaceType|TypeID|         Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|                Code|StatusDate|ReceivedDate|closeTime|All Households|Families|Families with Children|Families without Children|\n",
      "+-------+-----------+-----------+----------------+------+-------------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|10467.0|  3976078.0|  7481721.0|ENTIRE APARTMENT|   3.0|NON EMERGENCY|           28.0|PAINT/PLASTER|          198.0|         WALL|2527.0|             CRACKED|03/17/2016|  11/15/2007|   3045.0|       39372.0| 46530.0|               40961.0|                  52038.0|\n",
      "|10467.0|  3976078.0|  7481744.0|         KITCHEN|   3.0|NON EMERGENCY|           28.0|PAINT/PLASTER|          198.0|         WALL|2529.0|CHIPPED/PEELING/F...|03/17/2016|  11/15/2007|   3045.0|       39372.0| 46530.0|               40961.0|                  52038.0|\n",
      "|10467.0|  3976078.0|  7481764.0|ENTIRE APARTMENT|   1.0|    EMERGENCY|           11.0|      GENERAL|           75.0| CERAMIC-TILE|2573.0|MISSING FROM FLOO...|03/17/2016|  11/15/2007|   3045.0|       39372.0| 46530.0|               40961.0|                  52038.0|\n",
      "|11213.0|  4034091.0|  7591872.0|         KITCHEN|   3.0|NON EMERGENCY|           11.0|      GENERAL|           73.0|      CABINET| 679.0|  DAMAGED OR MISSING|03/18/2016|  12/12/2007|   3019.0|       44831.0| 51262.0|               45618.0|                  61229.0|\n",
      "|11213.0|  4034091.0|  7591876.0|ENTIRE APARTMENT|   3.0|NON EMERGENCY|           28.0|PAINT/PLASTER|          198.0|         WALL|2527.0|             CRACKED|03/18/2016|  12/12/2007|   3019.0|       44831.0| 51262.0|               45618.0|                  61229.0|\n",
      "|11213.0|  4034091.0|  7591877.0|ENTIRE APARTMENT|   1.0|    EMERGENCY|           11.0|      GENERAL|          112.0|         MOLD|2490.0|                MOLD|03/18/2016|  12/12/2007|   3019.0|       44831.0| 51262.0|               45618.0|                  61229.0|\n",
      "|11213.0|  4034091.0|  7591878.0|        ENTRANCE|   1.0|    EMERGENCY|           11.0|      GENERAL|           77.0|        DOORS|2472.0|APARTMENT DOOR BR...|03/18/2016|  12/12/2007|   3019.0|       44831.0| 51262.0|               45618.0|                  61229.0|\n",
      "|11216.0|  3885182.0|  7233153.0|         KITCHEN|   3.0|NON EMERGENCY|           28.0|PAINT/PLASTER|          197.0|      CEILING|2524.0|CHIPPED/PEELING/F...|03/18/2016|  08/16/2007|   3137.0|       67795.0| 79242.0|               90183.0|                  76490.0|\n",
      "|11375.0|  6072819.0|1.2686838E7| ENTIRE BUILDING|   1.0|    EMERGENCY|           12.0|      HEATING|          196.0| HEAT RELATED|1359.0|        NO HOT WATER|04/16/2021|  09/10/2012|   3140.0|       84713.0|109994.0|              131646.0|                  98606.0|\n",
      "|11375.0|  6262031.0|1.3067411E7| ENTIRE BUILDING|   1.0|    EMERGENCY|           12.0|      HEATING|          196.0| HEAT RELATED|1360.0|  NO HEAT& HOT WATER|04/16/2021|  01/19/2013|   3009.0|       84713.0|109994.0|              131646.0|                  98606.0|\n",
      "|11235.0|  4632570.0|  8957649.0| ENTIRE BUILDING|   1.0|    EMERGENCY|           12.0|      HEATING|          196.0| HEAT RELATED|1360.0|  NO HEAT& HOT WATER|03/13/2018|  02/13/2009|   3315.0|       54646.0| 71542.0|               70663.0|                  72566.0|\n",
      "|11216.0|  3662982.0|  6666290.0|     LIVING ROOM|   1.0|    EMERGENCY|           28.0|PAINT/PLASTER|          197.0|      CEILING|2522.0|                HOLE|03/17/2016|  01/29/2007|   3335.0|       67795.0| 79242.0|               90183.0|                  76490.0|\n",
      "|11216.0|  3662982.0|  6666292.0|       BEDROOM 3|   1.0|    EMERGENCY|           28.0|PAINT/PLASTER|          197.0|      CEILING|2522.0|                HOLE|03/17/2016|  01/29/2007|   3335.0|       67795.0| 79242.0|               90183.0|                  76490.0|\n",
      "|11216.0|  3662982.0|  6666301.0|     LIVING ROOM|   1.0|    EMERGENCY|           11.0|      GENERAL|          112.0|         MOLD|2490.0|                MOLD|03/17/2016|  01/29/2007|   3335.0|       67795.0| 79242.0|               90183.0|                  76490.0|\n",
      "|11221.0|  4632538.0|  8957609.0| ENTIRE BUILDING|   3.0|NON EMERGENCY|           11.0|      GENERAL|           77.0|        DOORS|2477.0|TO ROOF, NOT SELF...|05/09/2018|  02/13/2009|   3372.0|       56900.0| 61331.0|               57606.0|                  63711.0|\n",
      "|11221.0|  4632538.0|  8957611.0| ENTIRE BUILDING|   3.0|NON EMERGENCY|           28.0|PAINT/PLASTER|          198.0|         WALL|2530.0| DIRTY OR UNSANITARY|05/09/2018|  02/13/2009|   3372.0|       56900.0| 61331.0|               57606.0|                  63711.0|\n",
      "+-------+-----------+-----------+----------------+------+-------------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from df_view where closeTime > 3000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby MajorCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|       MajorCategory|count|\n",
      "+--------------------+-----+\n",
      "|              SAFETY| 6963|\n",
      "|            ELECTRIC|12613|\n",
      "|         DOOR/WINDOW|19180|\n",
      "|     FLOORING/STAIRS|12372|\n",
      "|    OUTSIDE BUILDING|  872|\n",
      "|UNSANITARY CONDITION|39395|\n",
      "|           APPLIANCE| 6975|\n",
      "|        CONSTRUCTION|    3|\n",
      "|            ELEVATOR|  582|\n",
      "|            NONCONST|   24|\n",
      "|      HEAT/HOT WATER|92654|\n",
      "|             GENERAL|12355|\n",
      "|       PAINT/PLASTER|29641|\n",
      "|            PLUMBING|25494|\n",
      "|             HEATING|  244|\n",
      "|          WATER LEAK|15956|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('MajorCategory').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby Zipcode\n",
    "https://hendra-herviawan.github.io/pyspark-groupby-and-aggregate-functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|    Zip|count|\n",
      "+-------+-----+\n",
      "|11226.0|10137|\n",
      "|10467.0| 8093|\n",
      "|10458.0| 8036|\n",
      "|10453.0| 7849|\n",
      "|10457.0| 7430|\n",
      "|10452.0| 7147|\n",
      "|10468.0| 6939|\n",
      "|10456.0| 6569|\n",
      "|11213.0| 5839|\n",
      "|10031.0| 5797|\n",
      "|11225.0| 5488|\n",
      "|10032.0| 5158|\n",
      "|11212.0| 5091|\n",
      "|11207.0| 4703|\n",
      "|10460.0| 4603|\n",
      "|11233.0| 4549|\n",
      "|10472.0| 4162|\n",
      "|10462.0| 4043|\n",
      "|10033.0| 3989|\n",
      "|11203.0| 3906|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by zipcode\n",
    "df_zip = df.groupby('Zip').count()\n",
    "\n",
    "# then order by the count for each zipcode, sort by desc to show the zipcodes with the highest complaint counts in the dataset\n",
    "df_zip.orderBy(df_zip['count'].desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows=275323\n"
     ]
    }
   ],
   "source": [
    "# distinct row count\n",
    "print('rows={}'.format(df.distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows=275323\n"
     ]
    }
   ],
   "source": [
    "# row count\n",
    "print('rows={}'.format(df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------------+--------------+-----------+---------+--------------------+------------------+--------------------+------------------+-----------+---------+---------------+-----------------+--------------+-------------------+-------------+---------------------------+------------------------------+\n",
      "|Zip_miss|ComplaintID_miss|ProblemID_miss|SpaceType_miss|TypeID_miss|Type_miss|MajorCategoryID_miss|MajorCategory_miss|MinorCategoryID_miss|MinorCategory_miss|CodeID_miss|Code_miss|StatusDate_miss|ReceivedDate_miss|closeTime_miss|All Households_miss|Families_miss|Families with Children_miss|Families without Children_miss|\n",
      "+--------+----------------+--------------+--------------+-----------+---------+--------------------+------------------+--------------------+------------------+-----------+---------+---------------+-----------------+--------------+-------------------+-------------+---------------------------+------------------------------+\n",
      "|     0.0|             0.0|           0.0|           0.0|        0.0|      0.0|                 0.0|               0.0|                 0.0|               0.0|        0.0|      0.0|            0.0|              0.0|           0.0|                0.0|          0.0|                        0.0|                           0.0|\n",
      "+--------+----------------+--------------+--------------+-----------+---------+--------------------+------------------+--------------------+------------------+-----------+---------+---------------+-----------------+--------------+-------------------+-------------+---------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for each field, compute missing percentage\n",
    "\n",
    "df.agg(*[\n",
    "    (1 - F.count(c) / F.count('*')).alias(c + '_miss')\n",
    "    for c in df.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now working through classification_wisc_breast_cancer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.sql.functions import col \n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param init (you will need to update data_path)\n",
    "infile = 'join_df_Mod4_April_16_edit1.csv'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Mod4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "df = spark.read.csv(infile, inferSchema=True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|    Zip|ComplaintID|ProblemID|       SpaceType|TypeID|     Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|                Code|StatusDate|ReceivedDate|closeTime|All Households|Families|Families with Children|Families without Children|\n",
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|11432.0|  2397487.0|3768602.0|ENTIRE APARTMENT|   2.0|HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 886.0|             ROACHES|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|3768603.0|           OTHER|   2.0|HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 884.0|                MICE|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|3768604.0| ENTIRE BUILDING|   2.0|HAZARDOUS|           28.0|PAINT/PLASTER|          198.0|         WALL|1400.0|PAINT DIRTY AND U...|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|3768605.0|           OTHER|   1.0|EMERGENCY|           13.0|     NONCONST|          101.0|      RUBBISH|1309.0|               OTHER|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|3768606.0|        BATHROOM|   1.0|EMERGENCY|            9.0|     PLUMBING|           68.0| WATER SUPPLY|1282.0|               OTHER|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 1: Using StringIndexer on MajorCategory\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StringIndexer.html\n",
    "### StringIndexer\n",
    "StringIndexer encodes a string column of labels to a column of label indices. StringIndexer can encode multiple columns. The indices are in [0, numLabels), and four ordering options are supported: “frequencyDesc”: descending order by label frequency (most frequent label assigned 0), “frequencyAsc”: ascending order by label frequency (least frequent label assigned 0), “alphabetDesc”: descending alphabetical order, and “alphabetAsc”: ascending alphabetical order (default = “frequencyDesc”). Note that in case of equal frequency when under “frequencyDesc”/”frequencyAsc”, the strings are further sorted by alphabet.\n",
    "\n",
    "The unseen labels will be put at index numLabels if user chooses to keep them. If the input column is numeric, we cast it to string and index the string values. When downstream pipeline components such as Estimator or Transformer make use of this string-indexed label, you must set the input column of the component to this string-indexed column name. In many cases, you can set the input column with setInputCol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+---------------------+\n",
      "|    Zip|ComplaintID|ProblemID|       SpaceType|TypeID|     Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|                Code|StatusDate|ReceivedDate|closeTime|All Households|Families|Families with Children|Families without Children|MajorCategory_indexed|\n",
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+---------------------+\n",
      "|11432.0|  2397487.0|3768602.0|ENTIRE APARTMENT|   2.0|HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 886.0|             ROACHES|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|                 14.0|\n",
      "|11432.0|  2397487.0|3768603.0|           OTHER|   2.0|HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 884.0|                MICE|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|                 14.0|\n",
      "|11432.0|  2397487.0|3768604.0| ENTIRE BUILDING|   2.0|HAZARDOUS|           28.0|PAINT/PLASTER|          198.0|         WALL|1400.0|PAINT DIRTY AND U...|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|                  2.0|\n",
      "|11432.0|  2397487.0|3768605.0|           OTHER|   1.0|EMERGENCY|           13.0|     NONCONST|          101.0|      RUBBISH|1309.0|               OTHER|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|                 14.0|\n",
      "|11432.0|  2397487.0|3768606.0|        BATHROOM|   1.0|EMERGENCY|            9.0|     PLUMBING|           68.0| WATER SUPPLY|1282.0|               OTHER|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|                  3.0|\n",
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://spark.apache.org/docs/latest/ml-features.html#stringindexer\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"MajorCategory\", outputCol=\"MajorCategory_indexed\") #, stringOrderType=\"frequencyDesc\")\n",
    "\n",
    "indexed = stringIndexer.fit(df).transform(df)\n",
    "\n",
    "indexed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: I'm also going to do this StringIndexer on Zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------------+------+-------------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+-----------+\n",
      "|    Zip|ComplaintID|ProblemID|       SpaceType|TypeID|         Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|                Code|StatusDate|ReceivedDate|closeTime|All Households|Families|Families with Children|Families without Children|Zip_indexed|\n",
      "+-------+-----------+---------+----------------+------+-------------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+-----------+\n",
      "|11432.0|  2397487.0|3768602.0|ENTIRE APARTMENT|   2.0|    HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 886.0|             ROACHES|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|       63.0|\n",
      "|11432.0|  2397487.0|3768603.0|           OTHER|   2.0|    HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 884.0|                MICE|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|       63.0|\n",
      "|11432.0|  2397487.0|3768604.0| ENTIRE BUILDING|   2.0|    HAZARDOUS|           28.0|PAINT/PLASTER|          198.0|         WALL|1400.0|PAINT DIRTY AND U...|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|       63.0|\n",
      "|11432.0|  2397487.0|3768605.0|           OTHER|   1.0|    EMERGENCY|           13.0|     NONCONST|          101.0|      RUBBISH|1309.0|               OTHER|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|       63.0|\n",
      "|11432.0|  2397487.0|3768606.0|        BATHROOM|   1.0|    EMERGENCY|            9.0|     PLUMBING|           68.0| WATER SUPPLY|1282.0|               OTHER|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|       63.0|\n",
      "|11432.0|  2397487.0|3768607.0|ENTIRE APARTMENT|   2.0|    HAZARDOUS|           28.0|PAINT/PLASTER|          198.0|         WALL|1366.0|          LARGE HOLE|09/12/2007|  06/20/2004|   1179.0|       62148.0| 67198.0|               55290.0|                  79468.0|       63.0|\n",
      "|11230.0|  2784991.0|4559325.0| ENTIRE BUILDING|   1.0|    EMERGENCY|           12.0|      HEATING|          196.0| HEAT RELATED|1358.0|             NO HEAT|04/21/2005|  03/03/2005|     49.0|       53070.0| 66240.0|               61640.0|                  72023.0|       30.0|\n",
      "|10467.0|  3976078.0|7481721.0|ENTIRE APARTMENT|   3.0|NON EMERGENCY|           28.0|PAINT/PLASTER|          198.0|         WALL|2527.0|             CRACKED|03/17/2016|  11/15/2007|   3045.0|       39372.0| 46530.0|               40961.0|                  52038.0|        1.0|\n",
      "|10467.0|  3976078.0|7481733.0|         BEDROOM|   1.0|    EMERGENCY|           11.0|      GENERAL|          112.0|         MOLD|2490.0|                MOLD|04/15/2008|  11/15/2007|    152.0|       39372.0| 46530.0|               40961.0|                  52038.0|        1.0|\n",
      "|10467.0|  3976078.0|7481744.0|         KITCHEN|   3.0|NON EMERGENCY|           28.0|PAINT/PLASTER|          198.0|         WALL|2529.0|CHIPPED/PEELING/F...|03/17/2016|  11/15/2007|   3045.0|       39372.0| 46530.0|               40961.0|                  52038.0|        1.0|\n",
      "+-------+-----------+---------+----------------+------+-------------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://spark.apache.org/docs/latest/ml-features.html#stringindexer\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"Zip\", outputCol=\"Zip_indexed\") #, stringOrderType=\"frequencyDesc\")\n",
    "\n",
    "indexed = stringIndexer.fit(df).transform(df)\n",
    "\n",
    "indexed.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2: Using MajorCategoryID and passing that into OneHotEncoder\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-features.html#stringindexer\n",
    "\n",
    "### OneHotEncoder\n",
    "One-hot encoding maps a categorical feature, represented as a label index, to a binary vector with at most a single one-value indicating the presence of a specific feature value from among the set of all feature values. This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features. For string type input data, it is common to encode categorical features using StringIndexer first.\n",
    "\n",
    "OneHotEncoder can transform multiple columns, returning an one-hot-encoded output vector column for each input column. It is common to merge these vectors into a single feature vector using VectorAssembler.\n",
    "\n",
    "OneHotEncoder supports the handleInvalid parameter to choose how to handle invalid input during transforming data. Available options include ‘keep’ (any invalid inputs are assigned to an extra categorical index) and ‘error’ (throw an error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------------+------+-------------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+-----------+-------------------+----------------+\n",
      "|    Zip|ComplaintID|ProblemID|       SpaceType|TypeID|         Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|                Code|StatusDate|ReceivedDate|closeTime|All Households|Families|Families with Children|Families without Children|Zip_indexed|MajorCategoryID_Vec|         Zip_Vec|\n",
      "+-------+-----------+---------+----------------+------+-------------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+-----------+-------------------+----------------+\n",
      "|11432.0|  2397487.0|3768602.0|ENTIRE APARTMENT|   2.0|    HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 886.0|             ROACHES|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|       63.0|    (65,[13],[1.0])|(177,[63],[1.0])|\n",
      "|11432.0|  2397487.0|3768603.0|           OTHER|   2.0|    HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 884.0|                MICE|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|       63.0|    (65,[13],[1.0])|(177,[63],[1.0])|\n",
      "|11432.0|  2397487.0|3768604.0| ENTIRE BUILDING|   2.0|    HAZARDOUS|           28.0|PAINT/PLASTER|          198.0|         WALL|1400.0|PAINT DIRTY AND U...|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|       63.0|    (65,[28],[1.0])|(177,[63],[1.0])|\n",
      "|11432.0|  2397487.0|3768605.0|           OTHER|   1.0|    EMERGENCY|           13.0|     NONCONST|          101.0|      RUBBISH|1309.0|               OTHER|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|       63.0|    (65,[13],[1.0])|(177,[63],[1.0])|\n",
      "|11432.0|  2397487.0|3768606.0|        BATHROOM|   1.0|    EMERGENCY|            9.0|     PLUMBING|           68.0| WATER SUPPLY|1282.0|               OTHER|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|       63.0|     (65,[9],[1.0])|(177,[63],[1.0])|\n",
      "|11432.0|  2397487.0|3768607.0|ENTIRE APARTMENT|   2.0|    HAZARDOUS|           28.0|PAINT/PLASTER|          198.0|         WALL|1366.0|          LARGE HOLE|09/12/2007|  06/20/2004|   1179.0|       62148.0| 67198.0|               55290.0|                  79468.0|       63.0|    (65,[28],[1.0])|(177,[63],[1.0])|\n",
      "|11230.0|  2784991.0|4559325.0| ENTIRE BUILDING|   1.0|    EMERGENCY|           12.0|      HEATING|          196.0| HEAT RELATED|1358.0|             NO HEAT|04/21/2005|  03/03/2005|     49.0|       53070.0| 66240.0|               61640.0|                  72023.0|       30.0|    (65,[12],[1.0])|(177,[30],[1.0])|\n",
      "|10467.0|  3976078.0|7481721.0|ENTIRE APARTMENT|   3.0|NON EMERGENCY|           28.0|PAINT/PLASTER|          198.0|         WALL|2527.0|             CRACKED|03/17/2016|  11/15/2007|   3045.0|       39372.0| 46530.0|               40961.0|                  52038.0|        1.0|    (65,[28],[1.0])| (177,[1],[1.0])|\n",
      "|10467.0|  3976078.0|7481733.0|         BEDROOM|   1.0|    EMERGENCY|           11.0|      GENERAL|          112.0|         MOLD|2490.0|                MOLD|04/15/2008|  11/15/2007|    152.0|       39372.0| 46530.0|               40961.0|                  52038.0|        1.0|    (65,[11],[1.0])| (177,[1],[1.0])|\n",
      "|10467.0|  3976078.0|7481744.0|         KITCHEN|   3.0|NON EMERGENCY|           28.0|PAINT/PLASTER|          198.0|         WALL|2529.0|CHIPPED/PEELING/F...|03/17/2016|  11/15/2007|   3045.0|       39372.0| 46530.0|               40961.0|                  52038.0|        1.0|    (65,[28],[1.0])| (177,[1],[1.0])|\n",
      "+-------+-----------+---------+----------------+------+-------------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+-----------+-------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"MajorCategoryID\",\"Zip_indexed\"], outputCols=[\"MajorCategoryID_Vec\", \"Zip_Vec\"])\n",
    "\n",
    "model = encoder.fit(indexed)\n",
    "\n",
    "encoded = model.transform(indexed)\n",
    "\n",
    "encoded.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: I think it makes more sense to go with Option 2. The last column of the dataframe variable 'encoded' contains the OneHotEncoded term for each row \n",
    "\n",
    "\n",
    "# Next step: run the assembler and transformed (following along from classification_wisc_breast_cancer.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use some of the fields as features\n",
    "# the VectorAssembler() takes in f1,f2,f3 and creates a new column called 'features', appending it to the end of the dataframe\n",
    "\n",
    "\n",
    "#assembler = VectorAssembler(inputCols=[\"f1\", \"f2\", \"f3\"], outputCol=\"features\") \n",
    "assembler = VectorAssembler(inputCols=[\"MajorCategoryID_Vec\", \"Zip_Vec\"], outputCol=\"features\") \n",
    "transformed = assembler.transform(encoded)\n",
    "\n",
    "# \".transform()\" tokenizes the dataset df, using the VectorAssembler that took in f1,f2,f3 and created a column called 'features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(VectorAssembler_ae73c3781590,\n",
       " DataFrame[Zip: double, ComplaintID: double, ProblemID: double, SpaceType: string, TypeID: double, Type: string, MajorCategoryID: double, MajorCategory: string, MinorCategoryID: double, MinorCategory: string, CodeID: double, Code: string, StatusDate: string, ReceivedDate: string, closeTime: double, All Households: double, Families: double, Families with Children: double, Families without Children: double, Zip_indexed: double, MajorCategoryID_Vec: vector, Zip_Vec: vector, features: vector])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler,transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to RDD\n",
    "# 'transformed is the DataFrame that has the 'features' column appended to the end of it'\n",
    "# so this is selecting the response variable 'col('diagnosis')' and the synthetic 'features' variable, then mapping it as a tuple \n",
    "dataRdd = transformed.select(col(\"closeTime\"), col(\"features\")).rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(53.0, SparseVector(242, {13: 1.0, 128: 1.0})),\n",
       " (53.0, SparseVector(242, {13: 1.0, 128: 1.0})),\n",
       " (53.0, SparseVector(242, {28: 1.0, 128: 1.0})),\n",
       " (53.0, SparseVector(242, {13: 1.0, 128: 1.0})),\n",
       " (53.0, SparseVector(242, {9: 1.0, 128: 1.0})),\n",
       " (1179.0, SparseVector(242, {28: 1.0, 128: 1.0})),\n",
       " (49.0, SparseVector(242, {12: 1.0, 95: 1.0})),\n",
       " (3045.0, SparseVector(242, {28: 1.0, 66: 1.0})),\n",
       " (152.0, SparseVector(242, {11: 1.0, 66: 1.0})),\n",
       " (3045.0, SparseVector(242, {28: 1.0, 66: 1.0})),\n",
       " (84.0, SparseVector(242, {10: 1.0, 66: 1.0})),\n",
       " (84.0, SparseVector(242, {9: 1.0, 66: 1.0})),\n",
       " (3045.0, SparseVector(242, {11: 1.0, 66: 1.0})),\n",
       " (52.0, SparseVector(242, {12: 1.0, 72: 1.0})),\n",
       " (2362.0, SparseVector(242, {28: 1.0, 118: 1.0}))]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at some data\n",
    "dataRdd.take(15)\n",
    "# the dataRdd is a set of tuples containing the diagnosis in the first place (index=0) and the DenseVector of 'features' (f1,f2,f3) in\n",
    "# the second place (index=1)\n",
    "\n",
    "\n",
    "# 244 = 65 + 179\n",
    "# the 65 is from the MajorCategoryID_Vec, it's the number of distinct MajorCategories in the dataset\n",
    "# the 179 is from the Zip_Vec, there are 179 distinct Zipcodes in the dataset\n",
    "\n",
    "# and the first numbers in the () are the closeTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ugh this is LogisticRegression...stopping now and moving to linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying this now with linear regression\n",
    "https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"comm\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df2 = spark.read.csv(\"join_df_Mod4_April_16_edit1.csv\",inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df.select('*').limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+----------------+------+-------------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|    Zip|ComplaintID|  ProblemID|       SpaceType|TypeID|         Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|                Code|StatusDate|ReceivedDate|closeTime|All Households|Families|Families with Children|Families without Children|\n",
      "+-------+-----------+-----------+----------------+------+-------------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "|11432.0|  2397487.0|  3768602.0|ENTIRE APARTMENT|   2.0|    HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 886.0|             ROACHES|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|  3768603.0|           OTHER|   2.0|    HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 884.0|                MICE|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|  3768604.0| ENTIRE BUILDING|   2.0|    HAZARDOUS|           28.0|PAINT/PLASTER|          198.0|         WALL|1400.0|PAINT DIRTY AND U...|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|  3768605.0|           OTHER|   1.0|    EMERGENCY|           13.0|     NONCONST|          101.0|      RUBBISH|1309.0|               OTHER|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|  3768606.0|        BATHROOM|   1.0|    EMERGENCY|            9.0|     PLUMBING|           68.0| WATER SUPPLY|1282.0|               OTHER|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11432.0|  2397487.0|  3768607.0|ENTIRE APARTMENT|   2.0|    HAZARDOUS|           28.0|PAINT/PLASTER|          198.0|         WALL|1366.0|          LARGE HOLE|09/12/2007|  06/20/2004|   1179.0|       62148.0| 67198.0|               55290.0|                  79468.0|\n",
      "|11230.0|  2784991.0|  4559325.0| ENTIRE BUILDING|   1.0|    EMERGENCY|           12.0|      HEATING|          196.0| HEAT RELATED|1358.0|             NO HEAT|04/21/2005|  03/03/2005|     49.0|       53070.0| 66240.0|               61640.0|                  72023.0|\n",
      "|10467.0|  3976078.0|  7481721.0|ENTIRE APARTMENT|   3.0|NON EMERGENCY|           28.0|PAINT/PLASTER|          198.0|         WALL|2527.0|             CRACKED|03/17/2016|  11/15/2007|   3045.0|       39372.0| 46530.0|               40961.0|                  52038.0|\n",
      "|10467.0|  3976078.0|  7481733.0|         BEDROOM|   1.0|    EMERGENCY|           11.0|      GENERAL|          112.0|         MOLD|2490.0|                MOLD|04/15/2008|  11/15/2007|    152.0|       39372.0| 46530.0|               40961.0|                  52038.0|\n",
      "|10467.0|  3976078.0|  7481744.0|         KITCHEN|   3.0|NON EMERGENCY|           28.0|PAINT/PLASTER|          198.0|         WALL|2529.0|CHIPPED/PEELING/F...|03/17/2016|  11/15/2007|   3045.0|       39372.0| 46530.0|               40961.0|                  52038.0|\n",
      "|10467.0|  3976078.0|  7481749.0|ENTIRE APARTMENT|   1.0|    EMERGENCY|           10.0|     ELECTRIC|           71.0|       WIRING|2460.0|EXPOSED, NOT SPAR...|02/07/2008|  11/15/2007|     84.0|       39372.0| 46530.0|               40961.0|                  52038.0|\n",
      "|10467.0|  3976078.0|  7481754.0|        BATHROOM|   1.0|    EMERGENCY|            9.0|     PLUMBING|          174.0|        SEWER| 846.0|RAW SEWAGE ACCUMU...|02/07/2008|  11/15/2007|     84.0|       39372.0| 46530.0|               40961.0|                  52038.0|\n",
      "|10467.0|  3976078.0|  7481764.0|ENTIRE APARTMENT|   1.0|    EMERGENCY|           11.0|      GENERAL|           75.0| CERAMIC-TILE|2573.0|MISSING FROM FLOO...|03/17/2016|  11/15/2007|   3045.0|       39372.0| 46530.0|               40961.0|                  52038.0|\n",
      "|10456.0|  4249936.0|  8067726.0|ENTIRE APARTMENT|   1.0|    EMERGENCY|           12.0|      HEATING|          196.0| HEAT RELATED|1359.0|        NO HOT WATER|06/30/2008|  05/09/2008|     52.0|       27917.0| 35269.0|               31099.0|                  42260.0|\n",
      "|11211.0|  4827199.0|  9542234.0|ENTIRE APARTMENT|   3.0|NON EMERGENCY|           28.0|PAINT/PLASTER|          198.0|         WALL|2529.0|CHIPPED/PEELING/F...|03/17/2016|  09/28/2009|   2362.0|       81228.0| 63639.0|               51088.0|                  80625.0|\n",
      "|10035.0|  5299242.0|1.0748798E7|ENTIRE APARTMENT|   1.0|    EMERGENCY|           12.0|      HEATING|          196.0| HEAT RELATED|1358.0|             NO HEAT|11/18/2010|  11/02/2010|     16.0|       29799.0| 36645.0|               28300.0|                  41685.0|\n",
      "|10035.0|  5446548.0|1.1036524E7|ENTIRE APARTMENT|   1.0|    EMERGENCY|           12.0|      HEATING|          196.0| HEAT RELATED|1358.0|             NO HEAT|01/29/2011|  01/17/2011|     12.0|       29799.0| 36645.0|               28300.0|                  41685.0|\n",
      "|11209.0|  6020766.0|1.2526588E7| ENTIRE BUILDING|   1.0|    EMERGENCY|           12.0|      HEATING|          196.0| HEAT RELATED|1359.0|        NO HOT WATER|10/05/2012|  06/27/2012|    100.0|       77917.0|101737.0|              105378.0|                  99627.0|\n",
      "|11220.0|  6102817.0|1.2773656E7| ENTIRE BUILDING|   1.0|    EMERGENCY|           12.0|      HEATING|          196.0| HEAT RELATED|1358.0|             NO HEAT|12/05/2012|  10/16/2012|     50.0|       50191.0| 47150.0|               39276.0|                  55656.0|\n",
      "|10029.0|  6156209.0|1.2869584E7| ENTIRE BUILDING|   1.0|    EMERGENCY|           12.0|      HEATING|          196.0| HEAT RELATED|1358.0|             NO HEAT|07/30/2014|  11/16/2012|    621.0|       33720.0| 39845.0|               32484.0|                  48020.0|\n",
      "+-------+-----------+-----------+----------------+------+-------------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Zip: double (nullable = true)\n",
      " |-- ComplaintID: double (nullable = true)\n",
      " |-- ProblemID: double (nullable = true)\n",
      " |-- SpaceType: string (nullable = true)\n",
      " |-- TypeID: double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- MajorCategoryID: double (nullable = true)\n",
      " |-- MajorCategory: string (nullable = true)\n",
      " |-- MinorCategoryID: double (nullable = true)\n",
      " |-- MinorCategory: string (nullable = true)\n",
      " |-- CodeID: double (nullable = true)\n",
      " |-- Code: string (nullable = true)\n",
      " |-- StatusDate: string (nullable = true)\n",
      " |-- ReceivedDate: string (nullable = true)\n",
      " |-- closeTime: double (nullable = true)\n",
      " |-- All Households: double (nullable = true)\n",
      " |-- Families: double (nullable = true)\n",
      " |-- Families with Children: double (nullable = true)\n",
      " |-- Families without Children: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cache()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zip</th>\n",
       "      <td>275323</td>\n",
       "      <td>10732.27465195425</td>\n",
       "      <td>512.8310972379447</td>\n",
       "      <td>10001.0</td>\n",
       "      <td>11697.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ComplaintID</th>\n",
       "      <td>275323</td>\n",
       "      <td>8640507.667964537</td>\n",
       "      <td>999659.684692623</td>\n",
       "      <td>1936914.0</td>\n",
       "      <td>1.0410825E7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProblemID</th>\n",
       "      <td>275323</td>\n",
       "      <td>1.7764211552580062E7</td>\n",
       "      <td>1926228.4741469913</td>\n",
       "      <td>2849265.0</td>\n",
       "      <td>2.1143445E7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SpaceType</th>\n",
       "      <td>275323</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AIR SHAFT</td>\n",
       "      <td>YARD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TypeID</th>\n",
       "      <td>275323</td>\n",
       "      <td>1.7902209404953455</td>\n",
       "      <td>1.0301193256644046</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <td>275323</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>NON EMERGENCY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MajorCategoryID</th>\n",
       "      <td>275323</td>\n",
       "      <td>46.04006566832411</td>\n",
       "      <td>21.300742681710446</td>\n",
       "      <td>8.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MajorCategory</th>\n",
       "      <td>275323</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>APPLIANCE</td>\n",
       "      <td>WATER LEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MinorCategoryID</th>\n",
       "      <td>275323</td>\n",
       "      <td>295.0284102672134</td>\n",
       "      <td>107.02721849161907</td>\n",
       "      <td>59.0</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MinorCategory</th>\n",
       "      <td>275323</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>APARTMENT ONLY</td>\n",
       "      <td>WIRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CodeID</th>\n",
       "      <td>275323</td>\n",
       "      <td>2578.7163767647453</td>\n",
       "      <td>482.61252276128874</td>\n",
       "      <td>614.0</td>\n",
       "      <td>2889.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Code</th>\n",
       "      <td>275323</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ACCUMULATION</td>\n",
       "      <td>WINDOW STUCK CLOSED OR OPEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StatusDate</th>\n",
       "      <td>275323</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/2014</td>\n",
       "      <td>12/31/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReceivedDate</th>\n",
       "      <td>275323</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/2014</td>\n",
       "      <td>12/31/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>closeTime</th>\n",
       "      <td>275323</td>\n",
       "      <td>13.838909208456975</td>\n",
       "      <td>42.05641735917829</td>\n",
       "      <td>-168.0</td>\n",
       "      <td>3372.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All Households</th>\n",
       "      <td>275323</td>\n",
       "      <td>54599.59264936093</td>\n",
       "      <td>24634.009101944248</td>\n",
       "      <td>21447.0</td>\n",
       "      <td>250001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Families</th>\n",
       "      <td>275323</td>\n",
       "      <td>66023.01643160942</td>\n",
       "      <td>39975.16494195124</td>\n",
       "      <td>25429.0</td>\n",
       "      <td>250001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Families with Children</th>\n",
       "      <td>275323</td>\n",
       "      <td>62668.236536722325</td>\n",
       "      <td>48919.9422881412</td>\n",
       "      <td>20114.0</td>\n",
       "      <td>250001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Families without Children</th>\n",
       "      <td>275323</td>\n",
       "      <td>70307.02262070368</td>\n",
       "      <td>33756.224330219695</td>\n",
       "      <td>29907.0</td>\n",
       "      <td>250001.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                0                     1                   2  \\\n",
       "summary                     count                  mean              stddev   \n",
       "Zip                        275323     10732.27465195425   512.8310972379447   \n",
       "ComplaintID                275323     8640507.667964537    999659.684692623   \n",
       "ProblemID                  275323  1.7764211552580062E7  1926228.4741469913   \n",
       "SpaceType                  275323                  None                None   \n",
       "TypeID                     275323    1.7902209404953455  1.0301193256644046   \n",
       "Type                       275323                  None                None   \n",
       "MajorCategoryID            275323     46.04006566832411  21.300742681710446   \n",
       "MajorCategory              275323                  None                None   \n",
       "MinorCategoryID            275323     295.0284102672134  107.02721849161907   \n",
       "MinorCategory              275323                  None                None   \n",
       "CodeID                     275323    2578.7163767647453  482.61252276128874   \n",
       "Code                       275323                  None                None   \n",
       "StatusDate                 275323                  None                None   \n",
       "ReceivedDate               275323                  None                None   \n",
       "closeTime                  275323    13.838909208456975   42.05641735917829   \n",
       "All Households             275323     54599.59264936093  24634.009101944248   \n",
       "Families                   275323     66023.01643160942   39975.16494195124   \n",
       "Families with Children     275323    62668.236536722325    48919.9422881412   \n",
       "Families without Children  275323     70307.02262070368  33756.224330219695   \n",
       "\n",
       "                                        3                            4  \n",
       "summary                               min                          max  \n",
       "Zip                               10001.0                      11697.0  \n",
       "ComplaintID                     1936914.0                  1.0410825E7  \n",
       "ProblemID                       2849265.0                  2.1143445E7  \n",
       "SpaceType                       AIR SHAFT                         YARD  \n",
       "TypeID                                1.0                          4.0  \n",
       "Type                            EMERGENCY                NON EMERGENCY  \n",
       "MajorCategoryID                       8.0                         65.0  \n",
       "MajorCategory                   APPLIANCE                   WATER LEAK  \n",
       "MinorCategoryID                      59.0                        400.0  \n",
       "MinorCategory              APARTMENT ONLY                       WIRING  \n",
       "CodeID                              614.0                       2889.0  \n",
       "Code                         ACCUMULATION  WINDOW STUCK CLOSED OR OPEN  \n",
       "StatusDate                     01/01/2014                   12/31/2020  \n",
       "ReceivedDate                   01/01/2014                   12/31/2020  \n",
       "closeTime                          -168.0                       3372.0  \n",
       "All Households                    21447.0                     250001.0  \n",
       "Families                          25429.0                     250001.0  \n",
       "Families with Children            20114.0                     250001.0  \n",
       "Families without Children         29907.0                     250001.0  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scatter matrix is a great way to roughly determine if we have a linear correlation between multiple independent variables.\n",
    "\n",
    "# from pandas.plotting import scatter_matrix \n",
    "# #https://stackoverflow.com/questions/55394041/how-can-i-solve-module-pandas-has-no-attribute-scatter-matrix-error\n",
    "\n",
    "# numeric_features = [t[0] for t in df.dtypes if t[1] == 'int' or t[1] == 'double']\n",
    "# sampled_data = df.select(numeric_features).sample(False, 0.8).toPandas()\n",
    "# axs = scatter_matrix(sampled_data, figsize=(10, 10))\n",
    "# n = len(sampled_data.columns)\n",
    "# for i in range(n):\n",
    "#     v = axs[i, 0]\n",
    "#     v.yaxis.label.set_rotation(0)\n",
    "#     v.yaxis.label.set_ha('right')\n",
    "#     v.set_yticks(())\n",
    "#     h = axs[n-1, i]\n",
    "#     h.xaxis.label.set_rotation(90)\n",
    "#     h.set_xticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "for i in df.columns:\n",
    "    if not( isinstance(df.select(i).take(1)[0][0], six.string_types)):\n",
    "        print( \"Correlation to closeTime for \", i, df.stat.corr('closeTime',i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "That's actually interesting to see. This obviously needs to be edited for better performance (if columns like Zip and MajorCategory were OneHotEncoded I think they would perform better). However, even still, seeing TypeID show 10% correlation is interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+---------------------+\n",
      "|    Zip|ComplaintID|ProblemID|       SpaceType|TypeID|     Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|                Code|StatusDate|ReceivedDate|closeTime|All Households|Families|Families with Children|Families without Children|MajorCategory_indexed|\n",
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+---------------------+\n",
      "|11432.0|  2397487.0|3768602.0|ENTIRE APARTMENT|   2.0|HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 886.0|             ROACHES|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|                 14.0|\n",
      "|11432.0|  2397487.0|3768603.0|           OTHER|   2.0|HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 884.0|                MICE|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|                 14.0|\n",
      "|11432.0|  2397487.0|3768604.0| ENTIRE BUILDING|   2.0|HAZARDOUS|           28.0|PAINT/PLASTER|          198.0|         WALL|1400.0|PAINT DIRTY AND U...|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|                  2.0|\n",
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+---------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://spark.apache.org/docs/latest/ml-features.html#stringindexer\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"MajorCategory\", outputCol=\"MajorCategory_indexed\") #, stringOrderType=\"frequencyDesc\")\n",
    "\n",
    "indexed = stringIndexer.fit(df2).transform(df2)\n",
    "\n",
    "indexed.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+-----------+\n",
      "|    Zip|ComplaintID|ProblemID|       SpaceType|TypeID|     Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|                Code|StatusDate|ReceivedDate|closeTime|All Households|Families|Families with Children|Families without Children|Zip_indexed|\n",
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+-----------+\n",
      "|11432.0|  2397487.0|3768602.0|ENTIRE APARTMENT|   2.0|HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 886.0|             ROACHES|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|       63.0|\n",
      "|11432.0|  2397487.0|3768603.0|           OTHER|   2.0|HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 884.0|                MICE|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|       63.0|\n",
      "|11432.0|  2397487.0|3768604.0| ENTIRE BUILDING|   2.0|HAZARDOUS|           28.0|PAINT/PLASTER|          198.0|         WALL|1400.0|PAINT DIRTY AND U...|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|       63.0|\n",
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://spark.apache.org/docs/latest/ml-features.html#stringindexer\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"Zip\", outputCol=\"Zip_indexed\") #, stringOrderType=\"frequencyDesc\")\n",
    "\n",
    "indexed = stringIndexer.fit(df2).transform(df2)\n",
    "\n",
    "indexed.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+-----------+-------------------+----------------+\n",
      "|    Zip|ComplaintID|ProblemID|       SpaceType|TypeID|     Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|                Code|StatusDate|ReceivedDate|closeTime|All Households|Families|Families with Children|Families without Children|Zip_indexed|MajorCategoryID_Vec|         Zip_Vec|\n",
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+-----------+-------------------+----------------+\n",
      "|11432.0|  2397487.0|3768602.0|ENTIRE APARTMENT|   2.0|HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 886.0|             ROACHES|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|       63.0|    (65,[13],[1.0])|(177,[63],[1.0])|\n",
      "|11432.0|  2397487.0|3768603.0|           OTHER|   2.0|HAZARDOUS|           13.0|     NONCONST|          106.0|       VERMIN| 884.0|                MICE|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|       63.0|    (65,[13],[1.0])|(177,[63],[1.0])|\n",
      "|11432.0|  2397487.0|3768604.0| ENTIRE BUILDING|   2.0|HAZARDOUS|           28.0|PAINT/PLASTER|          198.0|         WALL|1400.0|PAINT DIRTY AND U...|08/12/2004|  06/20/2004|     53.0|       62148.0| 67198.0|               55290.0|                  79468.0|       63.0|    (65,[28],[1.0])|(177,[63],[1.0])|\n",
      "+-------+-----------+---------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+----------+------------+---------+--------------+--------+----------------------+-------------------------+-----------+-------------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"MajorCategoryID\",\"Zip_indexed\"], outputCols=[\"MajorCategoryID_Vec\", \"Zip_Vec\"])\n",
    "\n",
    "model = encoder.fit(indexed)\n",
    "\n",
    "encoded = model.transform(indexed)\n",
    "\n",
    "encoded.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|            features|closeTime|\n",
      "+--------------------+---------+\n",
      "|[11432.0,2397487....|     53.0|\n",
      "|[11432.0,2397487....|     53.0|\n",
      "|[11432.0,2397487....|     53.0|\n",
      "|[11432.0,2397487....|     53.0|\n",
      "|[11432.0,2397487....|     53.0|\n",
      "|[11432.0,2397487....|   1179.0|\n",
      "|[11230.0,2784991....|     49.0|\n",
      "|[10467.0,3976078....|   3045.0|\n",
      "|[10467.0,3976078....|    152.0|\n",
      "|[10467.0,3976078....|   3045.0|\n",
      "|[10467.0,3976078....|     84.0|\n",
      "|[10467.0,3976078....|     84.0|\n",
      "|[10467.0,3976078....|   3045.0|\n",
      "|[10456.0,4249936....|     52.0|\n",
      "|[11211.0,4827199....|   2362.0|\n",
      "|[10035.0,5299242....|     16.0|\n",
      "|[10035.0,5446548....|     12.0|\n",
      "|[11209.0,6020766....|    100.0|\n",
      "|[11220.0,6102817....|     50.0|\n",
      "|[10029.0,6156209....|    621.0|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['Zip', 'ComplaintID', 'ProblemID', 'TypeID', 'MajorCategoryID', 'MinorCategoryID', 'CodeID', 'All Households', 'Families', 'Families with Children', 'Families without Children'], outputCol = 'features')\n",
    "\n",
    "vhouse_df = vectorAssembler.transform(df2)\n",
    "vhouse_df = vhouse_df.select(['features', 'closeTime'])\n",
    "\n",
    "vhouse_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = vhouse_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.0001516852538944442,0.0,-1.9405492493167985e-06,3.0664923939159556,-0.08321899215320436,0.0,-0.0009994276615069389,1.0649955413826916e-05,0.0,0.0,0.0]\n",
      "Intercept: 50.301184577481756\n"
     ]
    }
   ],
   "source": [
    "#Option 1\n",
    "\n",
    "\n",
    "# Linear Regression\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='closeTime', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 42.064267\n",
      "r2: 0.020839\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|         closeTime|\n",
      "+-------+------------------+\n",
      "|  count|            192824|\n",
      "|   mean|13.865213873791644|\n",
      "| stddev| 42.50962975662086|\n",
      "|    min|            -168.0|\n",
      "|    max|            3335.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE measures the differences between predicted values by the model and the actual values. However, RMSE alone is meaningless until we compare with the actual closeTime value, such as mean, min and max. \n",
    "\n",
    "### Interpretation? \n",
    "\n",
    "I'm guessing this means the linear model we built is pretty bad. R2 of 0.020839 means that roughly 2% of the variability in closeTime can be explained using the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+--------------------+\n",
      "|        prediction|closeTime|            features|\n",
      "+------------------+---------+--------------------+\n",
      "| 22.33798946858534|      6.0|[10001.0,7036561....|\n",
      "|16.573003607326875|      1.0|[10001.0,7055145....|\n",
      "| 25.39223056385068|      6.0|[10001.0,7066834....|\n",
      "| 22.77454386618384|      6.0|[10001.0,7066834....|\n",
      "|  14.4697152669263|     53.0|[10001.0,7525334....|\n",
      "+------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "R Squared (R2) on test data = 0.0215144\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\",\"closeTime\",\"features\").show(5)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"closeTime\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 40.5343\n"
     ]
    }
   ],
   "source": [
    "test_result = lr_model.evaluate(test_df)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numIterations: 11\n",
      "objectiveHistory: [0.5, 0.49363459502247903, 0.4912857186929599, 0.491096842856474, 0.4910389319933901, 0.49101920513001057, 0.4909560450922158, 0.4909261916755941, 0.4909170348964062, 0.49087468340101764, 0.4908524079510636]\n",
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|  677.7265402216044|\n",
      "| -1.480573265228145|\n",
      "|-1.4805713246788912|\n",
      "| -10.03157264336572|\n",
      "|-10.209000450850198|\n",
      "|-10.645814955710492|\n",
      "| -10.03156488116872|\n",
      "|-10.031562940619466|\n",
      "|-13.593094113705057|\n",
      "|-16.323737374062464|\n",
      "| -14.64706246543307|\n",
      "| -9.467656920632997|\n",
      "| -16.10753949369103|\n",
      "|-14.891979491574645|\n",
      "|-17.083755577330724|\n",
      "|  382.5648083775758|\n",
      "|-10.612802239427353|\n",
      "|-10.490130418631793|\n",
      "| -7.311381141100391|\n",
      "|-13.304912889375828|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+--------------------+\n",
      "|        prediction|closeTime|            features|\n",
      "+------------------+---------+--------------------+\n",
      "| 22.33798946858534|      6.0|[10001.0,7036561....|\n",
      "|16.573003607326875|      1.0|[10001.0,7055145....|\n",
      "| 25.39223056385068|      6.0|[10001.0,7066834....|\n",
      "| 22.77454386618384|      6.0|[10001.0,7066834....|\n",
      "|  14.4697152669263|     53.0|[10001.0,7525334....|\n",
      "| 14.44968487277675|     23.0|[10001.0,7528706....|\n",
      "|14.412754985152453|      6.0|[10001.0,7661040....|\n",
      "|14.370586849964802|      2.0|[10001.0,7674345....|\n",
      "| 14.34431147067803|      0.0|[10001.0,7708426....|\n",
      "|19.931273761198767|      0.0|[10001.0,7708426....|\n",
      "|20.527268133961364|    125.0|[10001.0,7719730....|\n",
      "| 19.89802664449699|    125.0|[10001.0,7719730....|\n",
      "| 24.59936870908634|      9.0|[10001.0,7730812....|\n",
      "| 12.79042290997917|      6.0|[10001.0,8008630....|\n",
      "|18.420418346810834|     45.0|[10001.0,8087123....|\n",
      "|12.439292169614212|      3.0|[10001.0,8160811....|\n",
      "|22.279269159095428|      8.0|[10001.0,8731415....|\n",
      "|15.712298784156388|     66.0|[10001.0,8832553....|\n",
      "| 8.661269646264152|      2.0|[10001.0,9173831....|\n",
      "| 7.515470491052014|      1.0|[10001.0,9516590....|\n",
      "+------------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using our Linear Regression model to make some predictions:\n",
    "predictions = lr_model.transform(test_df)\n",
    "predictions.select(\"prediction\",\"closeTime\",\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 40.6957\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'closeTime')\n",
    "dt_model = dt.fit(train_df)\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "dt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"closeTime\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = dt_evaluator.evaluate(dt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(11, {0: 0.1598, 1: 0.1559, 3: 0.1161, 4: 0.2012, 5: 0.0713, 6: 0.0153, 7: 0.0, 8: 0.0405, 9: 0.2398})"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model.featureImportances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "None of the variables are particularly important in predicting the closeTime however some are greater than others\n",
    "\n",
    "For example, the index 7 item has 0.0 importance while index 9 item has the highest of 0.2398\n",
    "\n",
    "\n",
    "Need to actually go back and determine what each index represents...I can do that later by looking at what I fed into the VectorAssembler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "I'm going to keep on testing models here but it's starting to become clear that the variable selection process needs to be altered. It's obviously super important what we put into the model. \n",
    "\n",
    "It might make sense to put 100% of the variables into the model at first to see how that performs? Especially because some of the models we run have feature selection built into their algorithms so maybe it'd be best just to do that and then work to pair down the insignificant terms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-boosted tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+--------------------+\n",
      "|        prediction|closeTime|            features|\n",
      "+------------------+---------+--------------------+\n",
      "|50.984674801984916|      6.0|[10001.0,7036561....|\n",
      "| 4.322943505230454|      1.0|[10001.0,7055145....|\n",
      "|  23.0004734004963|      6.0|[10001.0,7066834....|\n",
      "|47.503186649662226|      6.0|[10001.0,7066834....|\n",
      "|21.848660001687364|     53.0|[10001.0,7525334....|\n",
      "+------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(featuresCol = 'features', labelCol = 'closeTime', maxIter=10)\n",
    "gbt_model = gbt.fit(train_df)\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "gbt_predictions.select('prediction', 'closeTime', 'features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 40.3526\n"
     ]
    }
   ],
   "source": [
    "gbt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"closeTime\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This performed equally bad to all the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now I'm going to go ahead and actually throw all the features in and see how that performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means\n",
    "MLlib Clustering ipynb from Mod5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------+---------+\n",
      "|features                                                                               |closeTime|\n",
      "+---------------------------------------------------------------------------------------+---------+\n",
      "|[11432.0,2397487.0,3768602.0,2.0,13.0,106.0,886.0,62148.0,67198.0,55290.0,79468.0]     |53.0     |\n",
      "|[11432.0,2397487.0,3768603.0,2.0,13.0,106.0,884.0,62148.0,67198.0,55290.0,79468.0]     |53.0     |\n",
      "|[11432.0,2397487.0,3768604.0,2.0,28.0,198.0,1400.0,62148.0,67198.0,55290.0,79468.0]    |53.0     |\n",
      "|[11432.0,2397487.0,3768605.0,1.0,13.0,101.0,1309.0,62148.0,67198.0,55290.0,79468.0]    |53.0     |\n",
      "|[11432.0,2397487.0,3768606.0,1.0,9.0,68.0,1282.0,62148.0,67198.0,55290.0,79468.0]      |53.0     |\n",
      "|[11432.0,2397487.0,3768607.0,2.0,28.0,198.0,1366.0,62148.0,67198.0,55290.0,79468.0]    |1179.0   |\n",
      "|[11230.0,2784991.0,4559325.0,1.0,12.0,196.0,1358.0,53070.0,66240.0,61640.0,72023.0]    |49.0     |\n",
      "|[10467.0,3976078.0,7481721.0,3.0,28.0,198.0,2527.0,39372.0,46530.0,40961.0,52038.0]    |3045.0   |\n",
      "|[10467.0,3976078.0,7481733.0,1.0,11.0,112.0,2490.0,39372.0,46530.0,40961.0,52038.0]    |152.0    |\n",
      "|[10467.0,3976078.0,7481744.0,3.0,28.0,198.0,2529.0,39372.0,46530.0,40961.0,52038.0]    |3045.0   |\n",
      "|[10467.0,3976078.0,7481749.0,1.0,10.0,71.0,2460.0,39372.0,46530.0,40961.0,52038.0]     |84.0     |\n",
      "|[10467.0,3976078.0,7481754.0,1.0,9.0,174.0,846.0,39372.0,46530.0,40961.0,52038.0]      |84.0     |\n",
      "|[10467.0,3976078.0,7481764.0,1.0,11.0,75.0,2573.0,39372.0,46530.0,40961.0,52038.0]     |3045.0   |\n",
      "|[10456.0,4249936.0,8067726.0,1.0,12.0,196.0,1359.0,27917.0,35269.0,31099.0,42260.0]    |52.0     |\n",
      "|[11211.0,4827199.0,9542234.0,3.0,28.0,198.0,2529.0,81228.0,63639.0,51088.0,80625.0]    |2362.0   |\n",
      "|[10035.0,5299242.0,1.0748798E7,1.0,12.0,196.0,1358.0,29799.0,36645.0,28300.0,41685.0]  |16.0     |\n",
      "|[10035.0,5446548.0,1.1036524E7,1.0,12.0,196.0,1358.0,29799.0,36645.0,28300.0,41685.0]  |12.0     |\n",
      "|[11209.0,6020766.0,1.2526588E7,1.0,12.0,196.0,1359.0,77917.0,101737.0,105378.0,99627.0]|100.0    |\n",
      "|[11220.0,6102817.0,1.2773656E7,1.0,12.0,196.0,1358.0,50191.0,47150.0,39276.0,55656.0]  |50.0     |\n",
      "|[10029.0,6156209.0,1.2869584E7,1.0,12.0,196.0,1358.0,33720.0,39845.0,32484.0,48020.0]  |621.0    |\n",
      "+---------------------------------------------------------------------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assemble the features\n",
    "\n",
    "feats = ['Zip', 'ComplaintID', 'ProblemID', 'TypeID', 'MajorCategoryID', 'MinorCategoryID', 'CodeID', 'All Households', 'Families', 'Families with Children', 'Families without Children']\n",
    "assembler = VectorAssembler(inputCols=feats, outputCol=\"features\")\n",
    "\n",
    "dataset = assembler.transform(df)\n",
    "dataset.select(['features','closeTime']).show(truncate=False)\n",
    "#dataset.select(\"*\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a k-means model with k=2\n",
    "\n",
    "kmeans = KMeans().setK(2).setSeed(314)\n",
    "model = kmeans.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----------+\n",
      "|            features|closeTime|prediction|\n",
      "+--------------------+---------+----------+\n",
      "|[11432.0,2397487....|     53.0|         0|\n",
      "|[11432.0,2397487....|     53.0|         0|\n",
      "|[11432.0,2397487....|     53.0|         0|\n",
      "|[11432.0,2397487....|     53.0|         0|\n",
      "|[11432.0,2397487....|     53.0|         0|\n",
      "|[11432.0,2397487....|   1179.0|         0|\n",
      "|[11230.0,2784991....|     49.0|         0|\n",
      "|[10467.0,3976078....|   3045.0|         0|\n",
      "|[10467.0,3976078....|    152.0|         0|\n",
      "|[10467.0,3976078....|   3045.0|         0|\n",
      "|[10467.0,3976078....|     84.0|         0|\n",
      "|[10467.0,3976078....|     84.0|         0|\n",
      "|[10467.0,3976078....|   3045.0|         0|\n",
      "|[10456.0,4249936....|     52.0|         0|\n",
      "|[11211.0,4827199....|   2362.0|         0|\n",
      "|[10035.0,5299242....|     16.0|         0|\n",
      "|[10035.0,5446548....|     12.0|         0|\n",
      "|[11209.0,6020766....|    100.0|         0|\n",
      "|[11220.0,6102817....|     50.0|         0|\n",
      "|[10029.0,6156209....|    621.0|         0|\n",
      "+--------------------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions\n",
    "# note the transform() method does prediction\n",
    "\n",
    "predictions = model.transform(dataset)\n",
    "predictions.select(['features','closeTime','prediction']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.7786344236324183\n",
      "Cluster Centers: \n",
      "[array([1.07353882e+04, 7.79518686e+06, 1.61352546e+07, 1.81410997e+00,\n",
      "       4.53313726e+01, 2.92043470e+02, 2.56444802e+03, 5.51138692e+04,\n",
      "       6.67303528e+04, 6.35402681e+04, 7.09187095e+04]), array([1.07290812e+04, 9.50751532e+06, 1.94349597e+07, 1.76571904e+00,\n",
      "       4.67669404e+01, 2.98089929e+02, 2.59335079e+03, 5.40721223e+04,\n",
      "       6.52975332e+04, 6.17738328e+04, 6.96796428e+04])]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Model:\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "\n",
    "\n",
    "# cost = model.computeCost(dataset)\n",
    "# print(\"Within Set Sum of Squared Errors = \" + str(cost))\n",
    "# ...\n",
    "# https://stackoverflow.com/questions/65195312/kmeansmodel-object-has-no-attribute-computecost-in-apache-pyspark\n",
    "# apparently computeCost is deprecated in Spark 3.0.0\n",
    "# the docs for PySpark haven't been updated or published to detail how computeCost can now be calculated in PySpark 3.0 ... which is unfortunate\n",
    "# ...\n",
    "\n",
    "\n",
    "print(\"Cluster Centers: \")\n",
    "centers = model.clusterCenters()\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance: good but not great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model\n",
    "The Gaussian Mixture Model is a weighted combination of underlying Gaussian distributions, each with a fixed probability.\n",
    "The expectation-maximization algorithm is used in spark.mllib to estimate the parameters.\n",
    "There is a mean vector and a covariance matrix for each cluster.\n",
    "\n",
    "Fit Mixture of Two Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussians shown as a DataFrame: \n",
      "component mean vectors\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|mean                                                                                                                                                                                                        |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[10732.274651954249,8640507.667964537,1.776421155258006E7,1.7902209404953453,46.04006566832411,295.0284102672134,2578.7163767647453,54599.59264936093,66023.01643160942,62668.23653672232,70307.02262070368]|\n",
      "|[10732.274651954249,8640507.667964537,1.776421155258006E7,1.7902209404953453,46.04006566832411,295.0284102672134,2578.7163767647453,54599.59264936093,66023.01643160942,62668.23653672232,70307.02262070368]|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "component covariance matrices\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|cov                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|262994.7790681451    -5058263.647207098     ... (11 total)\n",
      "-5058263.647207098   9.99315855579236E11    ...\n",
      "-9537677.322824463   1.9250651861172444E12  ...\n",
      "6.411934611066662    -10361.493018643812    ...\n",
      "64.82781792409568    620830.7732505888      ...\n",
      "751.7403510512574    2627736.5533614154     ...\n",
      "1713.3007413926948   1.5813676881488288E7   ...\n",
      "393808.9036915277    -6.462560943724135E8   ...\n",
      "-1836056.2474203026  -9.462084779537052E8   ...\n",
      "-3026204.2373844767  -1.1475076029766347E9  ...\n",
      "-1228311.9685042659  -7.979166164506416E8   ...|\n",
      "|262994.7790681451    -5058263.647207098     ... (11 total)\n",
      "-5058263.647207098   9.99315855579236E11    ...\n",
      "-9537677.322824463   1.9250651861172444E12  ...\n",
      "6.411934611066662    -10361.493018643812    ...\n",
      "64.82781792409568    620830.7732505888      ...\n",
      "751.7403510512574    2627736.5533614154     ...\n",
      "1713.3007413926948   1.5813676881488288E7   ...\n",
      "393808.9036915277    -6.462560943724135E8   ...\n",
      "-1836056.2474203026  -9.462084779537052E8   ...\n",
      "-3026204.2373844767  -1.1475076029766347E9  ...\n",
      "-1228311.9685042659  -7.979166164506416E8   ...|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import GaussianMixture\n",
    "\n",
    "# reuse data from K-Means example above\n",
    "\n",
    "gmm = GaussianMixture().setK(2).setSeed(314)\n",
    "model = gmm.fit(dataset)\n",
    "\n",
    "print(\"Gaussians shown as a DataFrame: \")\n",
    "print(\"component mean vectors\")\n",
    "model.gaussiansDF.select(\"mean\").show(truncate=False)\n",
    "\n",
    "print(\"component covariance matrices\")\n",
    "model.gaussiansDF.select(\"cov\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "It's great that all of the models now run but it is painfully clear how important it is going to be to approparitely feature select for this project\n",
    "\n",
    "\n",
    "# Useful ipynbs will be the cal_housing assignment and the fidelity clustering assignment from Mod5\n",
    "\n",
    "looking at classification_wisc_breast_cancer from Mod4 will also be helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5559 Spark 3",
   "language": "python",
   "name": "ds5559_spark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
