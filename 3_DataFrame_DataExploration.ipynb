{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with PySpark DataFrames and exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Change width of Jupyter notebook\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"comm\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the dataset and show the number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that read.json can read a zipped JSON directly\n",
    "# df = spark.read.json('part-00000-a159c41a-bc58-4476-9b78-c437667f9c2b-c000.json.gz')\n",
    "\n",
    "df = spark.read.csv(\"NYC_Merged_Complaints_Data.csv\",inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276401"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()\n",
    "##2834417 when you don't have header=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- ComplaintID: integer (nullable = true)\n",
      " |-- ProblemID: integer (nullable = true)\n",
      " |-- UnitTypeID: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- SpaceTypeID : integer (nullable = true)\n",
      " |-- SpaceType: string (nullable = true)\n",
      " |-- TypeID: integer (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- MajorCategoryID: integer (nullable = true)\n",
      " |-- MajorCategory: string (nullable = true)\n",
      " |-- MinorCategoryID: integer (nullable = true)\n",
      " |-- MinorCategory: string (nullable = true)\n",
      " |-- CodeID: integer (nullable = true)\n",
      " |-- Code: string (nullable = true)\n",
      " |-- StatusID: integer (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- StatusDate: string (nullable = true)\n",
      " |-- StatusDescription: string (nullable = true)\n",
      " |-- BuildingID: integer (nullable = true)\n",
      " |-- BoroughID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- HouseNumber: string (nullable = true)\n",
      " |-- StreetName: string (nullable = true)\n",
      " |-- Zip: double (nullable = true)\n",
      " |-- Block: integer (nullable = true)\n",
      " |-- Lot: integer (nullable = true)\n",
      " |-- Apartment: string (nullable = true)\n",
      " |-- CommunityBoard: integer (nullable = true)\n",
      " |-- ReceivedDate: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the first 5 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+----------+------------+------------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+--------+------+----------+--------------------+----------+---------+-------+-----------+----------+-------+-----+---+---------+--------------+------------+\n",
      "|_c0|ComplaintID|ProblemID|UnitTypeID|    UnitType|SpaceTypeID |       SpaceType|TypeID|     Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|                Code|StatusID|Status|StatusDate|   StatusDescription|BuildingID|BoroughID|Borough|HouseNumber|StreetName|    Zip|Block|Lot|Apartment|CommunityBoard|ReceivedDate|\n",
      "+---+-----------+---------+----------+------------+------------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+--------+------+----------+--------------------+----------+---------+-------+-----------+----------+-------+-----+---+---------+--------------+------------+\n",
      "|  0|    2397487|  3768602|        20|   APARTMENT|          68|ENTIRE APARTMENT|     2|HAZARDOUS|             13|     NONCONST|            106|       VERMIN|   886|             ROACHES|       2| CLOSE|08/12/2004|The Department of...|    580051|        4| QUEENS|      88-47| 179 PLACE|11432.0| 9915| 11|      2FL|            12|  06/20/2004|\n",
      "|  1|    2397487|  3768603|        20|   APARTMENT|         159|           OTHER|     2|HAZARDOUS|             13|     NONCONST|            106|       VERMIN|   884|                MICE|       2| CLOSE|08/12/2004|The Department of...|    580051|        4| QUEENS|      88-47| 179 PLACE|11432.0| 9915| 11|      2FL|            12|  06/20/2004|\n",
      "|  2|    2397487|  3768604|        29|    BUILDING|         148| ENTIRE BUILDING|     2|HAZARDOUS|             28|PAINT/PLASTER|            198|         WALL|  1400|PAINT DIRTY AND U...|       2| CLOSE|08/12/2004|The Department of...|    580051|        4| QUEENS|      88-47| 179 PLACE|11432.0| 9915| 11|      2FL|            12|  06/20/2004|\n",
      "|  3|    2397487|  3768605|        24|PUBLIC PARTS|          83|           OTHER|     1|EMERGENCY|             13|     NONCONST|            101|      RUBBISH|  1309|               OTHER|       2| CLOSE|08/12/2004|The Department of...|    580051|        4| QUEENS|      88-47| 179 PLACE|11432.0| 9915| 11|      2FL|            12|  06/20/2004|\n",
      "|  4|    2397487|  3768606|        20|   APARTMENT|          62|        BATHROOM|     1|EMERGENCY|              9|     PLUMBING|             68| WATER SUPPLY|  1282|               OTHER|       2| CLOSE|08/12/2004|The Department of...|    580051|        4| QUEENS|      88-47| 179 PLACE|11432.0| 9915| 11|      2FL|            12|  06/20/2004|\n",
      "+---+-----------+---------+----------+------------+------------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+--------+------+----------+--------------------+----------+---------+-------+-----------+----------+-------+-----+---+---------+--------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.select('*').show(5)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location\n",
    "Count the number of records where the zipcode is 11216 (where I live)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3793"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sql, make dataframe accessible\n",
    "df.createOrReplaceTempView('zip')\n",
    "\n",
    "# this selects the 'Zip' column from the dataframe and saves it to the 'sqlDF' variable\n",
    "zipc = spark.sql(\"SELECT * FROM zip\") #.show(1)\n",
    "\n",
    "# .select() goes into the 'address' column (the only column in this reduced dataframe) and then selects the city from each row of this column\n",
    "zipcode = zipc.select('zip.Zip') #.show()\n",
    "\n",
    "# now that we have the cities isolated, we can return only the rows of the dataframe where the city is 'Houston'\n",
    "#houston = spark.sql(\"SELECT * FROM addr where address.city like '%Houston%'\") #.show()\n",
    "\n",
    "# also we need to use <<== 'Houston'>> instead of <<like '%[]%'>> to account for \"South Houston\"\n",
    "bedstuy = spark.sql(\"SELECT * FROM zip where zip == '11216'\") #.show()\n",
    "#houston.show()\n",
    "bedstuy.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of records in West Village (10014) ... should be a lot less "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "772"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sql, make dataframe accessible\n",
    "df.createOrReplaceTempView('zip')\n",
    "\n",
    "# this selects the 'Zip' column from the dataframe and saves it to the 'sqlDF' variable\n",
    "zipc = spark.sql(\"SELECT * FROM zip\") #.show(1)\n",
    "\n",
    "# .select() goes into the 'address' column (the only column in this reduced dataframe) and then selects the city from each row of this column\n",
    "zipcode = zipc.select('zip.Zip') #.show()\n",
    "\n",
    "# now that we have the cities isolated, we can return only the rows of the dataframe where the city is 'Houston'\n",
    "#houston = spark.sql(\"SELECT * FROM addr where address.city like '%Houston%'\") #.show()\n",
    "\n",
    "# also we need to use <<== 'Houston'>> instead of <<like '%[]%'>> to account for \"South Houston\"\n",
    "westvillage = spark.sql(\"SELECT * FROM zip where zip == '10014'\") #.show()\n",
    "#houston.show()\n",
    "westvillage.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate close time for each complaint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+----------+------------+------------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+--------+------+----------+--------------------+----------+---------+-------+-----------+----------+-------+-----+---+---------+--------------+------------+---------+\n",
      "|_c0|ComplaintID|ProblemID|UnitTypeID|    UnitType|SpaceTypeID |       SpaceType|TypeID|     Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|                Code|StatusID|Status|StatusDate|   StatusDescription|BuildingID|BoroughID|Borough|HouseNumber|StreetName|    Zip|Block|Lot|Apartment|CommunityBoard|ReceivedDate|closeTime|\n",
      "+---+-----------+---------+----------+------------+------------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+--------+------+----------+--------------------+----------+---------+-------+-----------+----------+-------+-----+---+---------+--------------+------------+---------+\n",
      "|  0|    2397487|  3768602|        20|   APARTMENT|          68|ENTIRE APARTMENT|     2|HAZARDOUS|             13|     NONCONST|            106|       VERMIN|   886|             ROACHES|       2| CLOSE|08/12/2004|The Department of...|    580051|        4| QUEENS|      88-47| 179 PLACE|11432.0| 9915| 11|      2FL|            12|  06/20/2004|       53|\n",
      "|  1|    2397487|  3768603|        20|   APARTMENT|         159|           OTHER|     2|HAZARDOUS|             13|     NONCONST|            106|       VERMIN|   884|                MICE|       2| CLOSE|08/12/2004|The Department of...|    580051|        4| QUEENS|      88-47| 179 PLACE|11432.0| 9915| 11|      2FL|            12|  06/20/2004|       53|\n",
      "|  2|    2397487|  3768604|        29|    BUILDING|         148| ENTIRE BUILDING|     2|HAZARDOUS|             28|PAINT/PLASTER|            198|         WALL|  1400|PAINT DIRTY AND U...|       2| CLOSE|08/12/2004|The Department of...|    580051|        4| QUEENS|      88-47| 179 PLACE|11432.0| 9915| 11|      2FL|            12|  06/20/2004|       53|\n",
      "|  3|    2397487|  3768605|        24|PUBLIC PARTS|          83|           OTHER|     1|EMERGENCY|             13|     NONCONST|            101|      RUBBISH|  1309|               OTHER|       2| CLOSE|08/12/2004|The Department of...|    580051|        4| QUEENS|      88-47| 179 PLACE|11432.0| 9915| 11|      2FL|            12|  06/20/2004|       53|\n",
      "|  4|    2397487|  3768606|        20|   APARTMENT|          62|        BATHROOM|     1|EMERGENCY|              9|     PLUMBING|             68| WATER SUPPLY|  1282|               OTHER|       2| CLOSE|08/12/2004|The Department of...|    580051|        4| QUEENS|      88-47| 179 PLACE|11432.0| 9915| 11|      2FL|            12|  06/20/2004|       53|\n",
      "+---+-----------+---------+----------+------------+------------+----------------+------+---------+---------------+-------------+---------------+-------------+------+--------------------+--------+------+----------+--------------------+----------+---------+-------+-----------+----------+-------+-----+---+---------+--------------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, col, split, explode\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "# preprocessing: convert dates to datetime format and calculate response variable - closing time\n",
    "timeDiff = (unix_timestamp('StatusDate', 'MM/dd/yyyy') - unix_timestamp('ReceivedDate', 'MM/dd/yyyy')) / 86400 #seconds per day\n",
    "timeDiff = timeDiff.cast(IntegerType())\n",
    "\n",
    "# add closing time (time for complaint to be resolved) to dataframe\n",
    "merged_df = df.withColumn(\"closeTime\", timeDiff)\n",
    "merged_df.show(5)\n",
    "#Reference: https://stackoverflow.com/questions/47701339/subtracting-two-date-columns-in-pyspark-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------------+------+-------------+---------------+-------------+---------------+-------------+------+--------------------+-------+----------+------------+---------+\n",
      "|ComplaintID|ProblemID|       SpaceType|TypeID|         Type|MajorCategoryID|MajorCategory|MinorCategoryID|MinorCategory|CodeID|                Code|    Zip|StatusDate|ReceivedDate|closeTime|\n",
      "+-----------+---------+----------------+------+-------------+---------------+-------------+---------------+-------------+------+--------------------+-------+----------+------------+---------+\n",
      "|    2397487|  3768602|ENTIRE APARTMENT|     2|    HAZARDOUS|             13|     NONCONST|            106|       VERMIN|   886|             ROACHES|11432.0|08/12/2004|  06/20/2004|       53|\n",
      "|    2397487|  3768603|           OTHER|     2|    HAZARDOUS|             13|     NONCONST|            106|       VERMIN|   884|                MICE|11432.0|08/12/2004|  06/20/2004|       53|\n",
      "|    2397487|  3768604| ENTIRE BUILDING|     2|    HAZARDOUS|             28|PAINT/PLASTER|            198|         WALL|  1400|PAINT DIRTY AND U...|11432.0|08/12/2004|  06/20/2004|       53|\n",
      "|    2397487|  3768605|           OTHER|     1|    EMERGENCY|             13|     NONCONST|            101|      RUBBISH|  1309|               OTHER|11432.0|08/12/2004|  06/20/2004|       53|\n",
      "|    2397487|  3768606|        BATHROOM|     1|    EMERGENCY|              9|     PLUMBING|             68| WATER SUPPLY|  1282|               OTHER|11432.0|08/12/2004|  06/20/2004|       53|\n",
      "|    2397487|  3768607|ENTIRE APARTMENT|     2|    HAZARDOUS|             28|PAINT/PLASTER|            198|         WALL|  1366|          LARGE HOLE|11432.0|09/12/2007|  06/20/2004|     1179|\n",
      "|    2784991|  4559325| ENTIRE BUILDING|     1|    EMERGENCY|             12|      HEATING|            196| HEAT RELATED|  1358|             NO HEAT|11230.0|04/21/2005|  03/03/2005|       49|\n",
      "|    3976078|  7481721|ENTIRE APARTMENT|     3|NON EMERGENCY|             28|PAINT/PLASTER|            198|         WALL|  2527|             CRACKED|10467.0|03/17/2016|  11/15/2007|     3045|\n",
      "+-----------+---------+----------------+------+-------------+---------------+-------------+---------------+-------------+------+--------------------+-------+----------+------------+---------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now reduce it\n",
    "reduced_df = merged_df[['ComplaintID', 'ProblemID', 'SpaceType', 'TypeID', 'Type', 'MajorCategoryID', 'MajorCategory', 'MinorCategoryID', 'MinorCategory', 'CodeID', 'Code', 'Zip', 'StatusDate', 'ReceivedDate', 'closeTime']]\n",
    "\n",
    "reduced_df.show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df.coalesce(1).write.option(\"header\",\"true\").option(\"sep\",\",\").mode(\"overwrite\").csv(\"output/path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10545"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sql, make dataframe accessible\n",
    "reduced_df.createOrReplaceTempView('addr')\n",
    "\n",
    "# updating the select to return everything and specifying the where clause to look for cases that took longer than 60 days to close \n",
    "greater30 = spark.sql(\"SELECT * FROM addr where closeTime >= '60'\")\n",
    "\n",
    "greater30.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "That is pretty insane. Over 100k complaints took longer than 2 months to close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location and closeTime\n",
    "Does it take longer to close in poor neighborhoods vs. in rich neighborhoods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# West Village\n",
    "spark.sql(\"SELECT * FROM addr where Zip == '10014' and closeTime >= '60'\").count() #.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SoHo\n",
    "spark.sql(\"SELECT * FROM addr where Zip == '10013' and closeTime >= '60'\").count() #.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chelsea\n",
    "spark.sql(\"SELECT * FROM addr where Zip == '10001' and closeTime >= '60'\").count() #.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TriBeCa\n",
    "spark.sql(\"SELECT * FROM addr where Zip == '10007' and closeTime >= '60'\").count() #.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bedford-Stuyvesant\n",
    "spark.sql(\"SELECT * FROM addr where Zip == '11216' and closeTime >= '60'\").count() #.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bronx, Morris Heights\n",
    "spark.sql(\"SELECT * FROM addr where Zip == '10453' and closeTime >= '60'\").count() #.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bronx, Melrose\n",
    "spark.sql(\"SELECT * FROM addr where Zip == '10456' and closeTime >= '60'\").count() #.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bronx, Bathgate\n",
    "spark.sql(\"SELECT * FROM addr where Zip == '10457' and closeTime >= '60'\").count() #.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Income\n",
    "It would be really great if we could pull in zip code level (household) income data and then show the relationship between income and time it takes for these complaints to be closed. After this cursory search by zipcodes, I think that the data we have will illustrate how low-income tenants suffer disproportionatley to high-income tenants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5559 Spark 3",
   "language": "python",
   "name": "ds5559_spark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
